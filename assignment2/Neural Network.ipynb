{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1],\n",
    "              [1, -2, 3]])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:5], train_y[:5])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:5], train_y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.226125, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.205280, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.323565, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.123341, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.224853, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.296457, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.019234, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.218654, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.218704, Train accuracy: 0.206333, val accuracy: 0.212000\n",
      "Loss: 2.174402, Train accuracy: 0.239222, val accuracy: 0.245000\n",
      "Loss: 1.993911, Train accuracy: 0.254111, val accuracy: 0.249000\n",
      "Loss: 1.963277, Train accuracy: 0.268889, val accuracy: 0.266000\n",
      "Loss: 1.964034, Train accuracy: 0.276889, val accuracy: 0.272000\n",
      "Loss: 1.958852, Train accuracy: 0.282556, val accuracy: 0.284000\n",
      "Loss: 1.796243, Train accuracy: 0.330778, val accuracy: 0.336000\n",
      "Loss: 1.816559, Train accuracy: 0.366667, val accuracy: 0.360000\n",
      "Loss: 1.756264, Train accuracy: 0.398111, val accuracy: 0.388000\n",
      "Loss: 1.730782, Train accuracy: 0.414111, val accuracy: 0.410000\n",
      "Loss: 1.676435, Train accuracy: 0.444667, val accuracy: 0.430000\n",
      "Loss: 1.526289, Train accuracy: 0.468111, val accuracy: 0.448000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-3)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fb13f4b248>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.190551, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.370391, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.244266, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.150608, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.154511, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.232421, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.166779, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.045176, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.214703, Train accuracy: 0.204556, val accuracy: 0.214000\n",
      "Loss: 2.044396, Train accuracy: 0.231889, val accuracy: 0.236000\n",
      "Loss: 2.118654, Train accuracy: 0.256222, val accuracy: 0.254000\n",
      "Loss: 1.991909, Train accuracy: 0.264222, val accuracy: 0.263000\n",
      "Loss: 2.063617, Train accuracy: 0.277889, val accuracy: 0.277000\n",
      "Loss: 2.078456, Train accuracy: 0.289111, val accuracy: 0.285000\n",
      "Loss: 1.963585, Train accuracy: 0.332889, val accuracy: 0.335000\n",
      "Loss: 1.787479, Train accuracy: 0.363556, val accuracy: 0.360000\n",
      "Loss: 1.555048, Train accuracy: 0.394222, val accuracy: 0.375000\n",
      "Loss: 1.876295, Train accuracy: 0.417889, val accuracy: 0.403000\n",
      "Loss: 1.887572, Train accuracy: 0.427222, val accuracy: 0.425000\n",
      "Loss: 1.693698, Train accuracy: 0.455000, val accuracy: 0.447000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-3)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2, learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.299315, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278028, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.293018, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.246114, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.281468, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.266762, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.234638, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.267515, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297086, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261969, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.275399, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228098, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.277722, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.309300, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.277368, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.305308, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.272417, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.181918, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291148, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.117211, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-3)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.334465, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327193, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301045, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.281082, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.311843, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.273265, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.268819, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.243616, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.212455, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.964681, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.054296, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.695140, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.023193, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.723346, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.225030, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.852488, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.818120, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.797909, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.761170, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.295354, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.734732, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.968858, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.952416, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.432232, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.457687, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.035145, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.719283, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.445595, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.660489, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.606382, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.287192, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.805711, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.550651, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.486419, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.249547, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.997179, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.490252, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 2.039351, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.859909, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.193265, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.632240, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.392340, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.470509, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.031045, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.241771, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.529709, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.437170, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.992508, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.612851, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.378023, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.831847, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.473726, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.471622, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.776907, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.662326, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.786534, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.257237, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.282906, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.483807, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.809352, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.743265, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.680297, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.630870, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.968383, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.625211, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.992515, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.213171, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.161674, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.432631, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.233146, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.987218, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.214349, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.548778, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.663107, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.199578, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.711541, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.941769, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.304413, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.060027, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.933902, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.688740, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.618307, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.607125, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.165852, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.554466, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.620399, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.590014, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.074256, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.418010, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.265188, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.248813, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.033228, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.234442, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.553977, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.328318, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.952722, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.354718, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.296185, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.548810, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.803371, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.452776, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.140498, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.342630, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.675162, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.356693, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.073252, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.494113, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.643045, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.389890, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.636989, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.310570, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.021876, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.609780, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.265544, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.626118, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.456926, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.358148, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.549455, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.330060, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.124151, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.187984, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.067396, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.800420, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.946894, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.372303, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.484882, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.432919, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.739882, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.284747, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.281767, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.228216, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.267046, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.043357, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.237148, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.267601, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.411644, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.271359, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.186069, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.507749, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.203464, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.185852, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.303732, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.324698, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.123889, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.219636, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.335275, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.173439, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.314237, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.339012, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.118034, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.374455, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.200391, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.241799, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.278784, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.176164, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 3.248931, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.042971, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: 2.422929, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.834378, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 0.501597, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.325345, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 0.168748, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.979882, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 0.769823, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 0.033236, Train accuracy: 0.800000, val accuracy: 0.133333\n",
      "Loss: 0.366908, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.740945, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.208775, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 0.061635, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 0.036388, Train accuracy: 1.000000, val accuracy: 0.066667\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 200, reg = 2e-3)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=2)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.246042, Train accuracy: 0.160000, val accuracy: 0.230000\n",
      "Loss: 2.326456, Train accuracy: 0.160000, val accuracy: 0.230000\n",
      "Loss: 2.046861, Train accuracy: 0.170000, val accuracy: 0.140000\n",
      "Loss: 2.348859, Train accuracy: 0.180000, val accuracy: 0.180000\n",
      "Loss: 1.813285, Train accuracy: 0.180000, val accuracy: 0.210000\n",
      "Loss: 2.549756, Train accuracy: 0.270000, val accuracy: 0.150000\n",
      "Loss: 1.973400, Train accuracy: 0.390000, val accuracy: 0.170000\n",
      "Loss: 2.283201, Train accuracy: 0.470000, val accuracy: 0.180000\n",
      "Loss: 1.736427, Train accuracy: 0.360000, val accuracy: 0.140000\n",
      "Loss: 2.465635, Train accuracy: 0.580000, val accuracy: 0.110000\n",
      "Loss: 1.413163, Train accuracy: 0.520000, val accuracy: 0.170000\n",
      "Loss: 2.328990, Train accuracy: 0.600000, val accuracy: 0.180000\n",
      "Loss: 1.152070, Train accuracy: 0.520000, val accuracy: 0.070000\n",
      "Loss: 1.756652, Train accuracy: 0.780000, val accuracy: 0.110000\n",
      "Loss: 1.304474, Train accuracy: 0.600000, val accuracy: 0.140000\n",
      "Loss: 0.600548, Train accuracy: 0.610000, val accuracy: 0.160000\n",
      "Loss: 3.259709, Train accuracy: 0.550000, val accuracy: 0.090000\n",
      "Loss: 0.255070, Train accuracy: 0.660000, val accuracy: 0.180000\n",
      "Loss: 2.298804, Train accuracy: 0.540000, val accuracy: 0.080000\n",
      "Loss: 14.197041, Train accuracy: 0.530000, val accuracy: 0.150000\n",
      "Loss: 4.370413, Train accuracy: 0.660000, val accuracy: 0.160000\n",
      "Loss: 0.569133, Train accuracy: 0.520000, val accuracy: 0.100000\n",
      "Loss: 0.744471, Train accuracy: 0.480000, val accuracy: 0.080000\n",
      "Loss: 41.063218, Train accuracy: 0.570000, val accuracy: 0.190000\n",
      "Loss: 26.387861, Train accuracy: 0.640000, val accuracy: 0.140000\n",
      "Loss: 1.915001, Train accuracy: 0.590000, val accuracy: 0.080000\n",
      "Loss: 5.075905, Train accuracy: 0.590000, val accuracy: 0.120000\n",
      "Loss: 1.754857, Train accuracy: 0.490000, val accuracy: 0.180000\n",
      "Loss: 43.678163, Train accuracy: 0.360000, val accuracy: 0.120000\n",
      "Loss: 18.804689, Train accuracy: 0.590000, val accuracy: 0.140000\n",
      "Loss: 3.383010, Train accuracy: 0.600000, val accuracy: 0.170000\n",
      "Loss: 3.855479, Train accuracy: 0.370000, val accuracy: 0.150000\n",
      "Loss: 10.653269, Train accuracy: 0.450000, val accuracy: 0.110000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\dlcourse\\dlcourse_ai\\assignments\\assignment2\\layers.py:20: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = np.mean(-np.log(prob))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 33.595292, Train accuracy: 0.500000, val accuracy: 0.190000\n",
      "Loss: 6.471151, Train accuracy: 0.560000, val accuracy: 0.270000\n",
      "Loss: 34.909163, Train accuracy: 0.520000, val accuracy: 0.140000\n",
      "Loss: 229.285542, Train accuracy: 0.540000, val accuracy: 0.160000\n",
      "Loss: 287.498706, Train accuracy: 0.420000, val accuracy: 0.170000\n",
      "Loss: 18.210207, Train accuracy: 0.490000, val accuracy: 0.090000\n",
      "Loss: 266.771570, Train accuracy: 0.650000, val accuracy: 0.190000\n",
      "Loss: 63.286783, Train accuracy: 0.590000, val accuracy: 0.160000\n",
      "Loss: 178.915763, Train accuracy: 0.460000, val accuracy: 0.130000\n",
      "Loss: 509.402303, Train accuracy: 0.540000, val accuracy: 0.080000\n",
      "Loss: 163.683023, Train accuracy: 0.560000, val accuracy: 0.140000\n",
      "Loss: inf, Train accuracy: 0.400000, val accuracy: 0.160000\n",
      "Loss: inf, Train accuracy: 0.480000, val accuracy: 0.210000\n",
      "Loss: 49.371413, Train accuracy: 0.390000, val accuracy: 0.070000\n",
      "Loss: 60.066411, Train accuracy: 0.490000, val accuracy: 0.130000\n",
      "Loss: 200.819078, Train accuracy: 0.560000, val accuracy: 0.180000\n",
      "Loss: 87.695552, Train accuracy: 0.530000, val accuracy: 0.230000\n",
      "Loss: 101.516008, Train accuracy: 0.410000, val accuracy: 0.150000\n",
      "Loss: inf, Train accuracy: 0.570000, val accuracy: 0.150000\n",
      "Loss: inf, Train accuracy: 0.730000, val accuracy: 0.180000\n",
      "Loss: inf, Train accuracy: 0.570000, val accuracy: 0.130000\n",
      "Loss: inf, Train accuracy: 0.600000, val accuracy: 0.170000\n",
      "Loss: inf, Train accuracy: 0.600000, val accuracy: 0.180000\n",
      "Loss: inf, Train accuracy: 0.510000, val accuracy: 0.160000\n",
      "Loss: 527.873975, Train accuracy: 0.620000, val accuracy: 0.170000\n",
      "Loss: 396.696191, Train accuracy: 0.680000, val accuracy: 0.150000\n",
      "Loss: inf, Train accuracy: 0.460000, val accuracy: 0.150000\n",
      "Loss: inf, Train accuracy: 0.730000, val accuracy: 0.150000\n",
      "Loss: inf, Train accuracy: 0.580000, val accuracy: 0.170000\n",
      "Loss: inf, Train accuracy: 0.730000, val accuracy: 0.120000\n",
      "Loss: inf, Train accuracy: 0.530000, val accuracy: 0.160000\n",
      "Loss: 594.821292, Train accuracy: 0.640000, val accuracy: 0.180000\n",
      "Loss: 678.013084, Train accuracy: 0.620000, val accuracy: 0.200000\n",
      "Loss: 747.097012, Train accuracy: 0.740000, val accuracy: 0.210000\n",
      "Loss: inf, Train accuracy: 0.530000, val accuracy: 0.150000\n",
      "Loss: inf, Train accuracy: 0.390000, val accuracy: 0.130000\n",
      "Loss: inf, Train accuracy: 0.400000, val accuracy: 0.200000\n",
      "Loss: 1883.929557, Train accuracy: 0.380000, val accuracy: 0.180000\n",
      "Loss: inf, Train accuracy: 0.660000, val accuracy: 0.190000\n",
      "Loss: inf, Train accuracy: 0.600000, val accuracy: 0.120000\n",
      "Loss: 3112.898819, Train accuracy: 0.460000, val accuracy: 0.200000\n",
      "Loss: 3355.852068, Train accuracy: 0.710000, val accuracy: 0.180000\n",
      "Loss: inf, Train accuracy: 0.510000, val accuracy: 0.220000\n",
      "Loss: inf, Train accuracy: 0.700000, val accuracy: 0.170000\n",
      "Loss: 4669.928024, Train accuracy: 0.670000, val accuracy: 0.160000\n",
      "Loss: 4965.675822, Train accuracy: 0.620000, val accuracy: 0.190000\n",
      "Loss: 5869.857885, Train accuracy: 0.590000, val accuracy: 0.090000\n",
      "Loss: inf, Train accuracy: 0.540000, val accuracy: 0.150000\n",
      "Loss: inf, Train accuracy: 0.750000, val accuracy: 0.120000\n",
      "Loss: 8084.403597, Train accuracy: 0.680000, val accuracy: 0.130000\n",
      "Loss: 9203.854623, Train accuracy: 0.630000, val accuracy: 0.170000\n",
      "Loss: inf, Train accuracy: 0.670000, val accuracy: 0.170000\n",
      "Loss: inf, Train accuracy: 0.800000, val accuracy: 0.140000\n",
      "Loss: inf, Train accuracy: 0.830000, val accuracy: 0.210000\n",
      "Loss: inf, Train accuracy: 0.670000, val accuracy: 0.150000\n",
      "Loss: inf, Train accuracy: 0.600000, val accuracy: 0.170000\n",
      "Loss: inf, Train accuracy: 0.570000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.630000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.640000, val accuracy: 0.170000\n",
      "Loss: inf, Train accuracy: 0.660000, val accuracy: 0.150000\n",
      "Loss: inf, Train accuracy: 0.680000, val accuracy: 0.140000\n",
      "Loss: inf, Train accuracy: 0.690000, val accuracy: 0.180000\n",
      "Loss: inf, Train accuracy: 0.540000, val accuracy: 0.220000\n",
      "Loss: 32394.011922, Train accuracy: 0.720000, val accuracy: 0.190000\n",
      "Loss: 36665.571182, Train accuracy: 0.480000, val accuracy: 0.190000\n",
      "Loss: inf, Train accuracy: 0.640000, val accuracy: 0.160000\n",
      "Loss: inf, Train accuracy: 0.540000, val accuracy: 0.190000\n"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 0.01\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 256\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = hidden_layer_size, reg = reg_strength)\n",
    "dataset = Dataset(train_X[:100], train_y[:100], val_X[:100], val_y[:100])\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=learning_rates, learning_rate_decay= learning_rate_decay, num_epochs=100, batch_size=2)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "#print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
