{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([[-10, 0, 10]]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([[1000, 0, 0]]))\n",
    "assert np.isclose(probs[0][0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.tensor([[-5., 5., 0.], [3., -2., 7.]])\n",
    "ind = torch.tensor([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0125)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(input, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012515777125719649"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([[-5, 5, 0], [3, -2, 7]]))\n",
    "linear_classifer.cross_entropy_loss(probs, np.array([1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([[1, 2, 2],[3, 0, -1]]), np.array([[1], [0]]))\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, np.array([[1], [0]])), np.array(np.array([[1, 2, 2],[3, 0, -1]]), np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.397260\n",
      "Epoch 1, loss: 2.330214\n",
      "Epoch 2, loss: 2.310426\n",
      "Epoch 3, loss: 2.304377\n",
      "Epoch 4, loss: 2.302885\n",
      "Epoch 5, loss: 2.302995\n",
      "Epoch 6, loss: 2.302474\n",
      "Epoch 7, loss: 2.301422\n",
      "Epoch 8, loss: 2.302158\n",
      "Epoch 9, loss: 2.301547\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1dab4627708>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1dab45c5fc8>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.127\n",
      "Epoch 0, loss: 2.301800\n",
      "Epoch 1, loss: 2.302558\n",
      "Epoch 2, loss: 2.302208\n",
      "Epoch 3, loss: 2.303309\n",
      "Epoch 4, loss: 2.302126\n",
      "Epoch 5, loss: 2.301524\n",
      "Epoch 6, loss: 2.301150\n",
      "Epoch 7, loss: 2.301359\n",
      "Epoch 8, loss: 2.302305\n",
      "Epoch 9, loss: 2.302205\n",
      "Epoch 10, loss: 2.301595\n",
      "Epoch 11, loss: 2.302039\n",
      "Epoch 12, loss: 2.301247\n",
      "Epoch 13, loss: 2.301836\n",
      "Epoch 14, loss: 2.302396\n",
      "Epoch 15, loss: 2.302299\n",
      "Epoch 16, loss: 2.301161\n",
      "Epoch 17, loss: 2.302058\n",
      "Epoch 18, loss: 2.302180\n",
      "Epoch 19, loss: 2.302339\n",
      "Epoch 20, loss: 2.302694\n",
      "Epoch 21, loss: 2.301872\n",
      "Epoch 22, loss: 2.302261\n",
      "Epoch 23, loss: 2.302209\n",
      "Epoch 24, loss: 2.302215\n",
      "Epoch 25, loss: 2.301591\n",
      "Epoch 26, loss: 2.301535\n",
      "Epoch 27, loss: 2.302557\n",
      "Epoch 28, loss: 2.301689\n",
      "Epoch 29, loss: 2.301815\n",
      "Epoch 30, loss: 2.301483\n",
      "Epoch 31, loss: 2.301553\n",
      "Epoch 32, loss: 2.301640\n",
      "Epoch 33, loss: 2.301991\n",
      "Epoch 34, loss: 2.302104\n",
      "Epoch 35, loss: 2.301741\n",
      "Epoch 36, loss: 2.302652\n",
      "Epoch 37, loss: 2.303017\n",
      "Epoch 38, loss: 2.301521\n",
      "Epoch 39, loss: 2.301504\n",
      "Epoch 40, loss: 2.301565\n",
      "Epoch 41, loss: 2.302361\n",
      "Epoch 42, loss: 2.302223\n",
      "Epoch 43, loss: 2.302385\n",
      "Epoch 44, loss: 2.302532\n",
      "Epoch 45, loss: 2.301950\n",
      "Epoch 46, loss: 2.301686\n",
      "Epoch 47, loss: 2.301881\n",
      "Epoch 48, loss: 2.302291\n",
      "Epoch 49, loss: 2.302069\n",
      "Epoch 50, loss: 2.301971\n",
      "Epoch 51, loss: 2.302533\n",
      "Epoch 52, loss: 2.302043\n",
      "Epoch 53, loss: 2.302003\n",
      "Epoch 54, loss: 2.301729\n",
      "Epoch 55, loss: 2.301272\n",
      "Epoch 56, loss: 2.302220\n",
      "Epoch 57, loss: 2.301765\n",
      "Epoch 58, loss: 2.301959\n",
      "Epoch 59, loss: 2.302229\n",
      "Epoch 60, loss: 2.301372\n",
      "Epoch 61, loss: 2.302540\n",
      "Epoch 62, loss: 2.301929\n",
      "Epoch 63, loss: 2.301502\n",
      "Epoch 64, loss: 2.302201\n",
      "Epoch 65, loss: 2.302674\n",
      "Epoch 66, loss: 2.302191\n",
      "Epoch 67, loss: 2.302989\n",
      "Epoch 68, loss: 2.301388\n",
      "Epoch 69, loss: 2.301621\n",
      "Epoch 70, loss: 2.301533\n",
      "Epoch 71, loss: 2.302804\n",
      "Epoch 72, loss: 2.301514\n",
      "Epoch 73, loss: 2.300962\n",
      "Epoch 74, loss: 2.302366\n",
      "Epoch 75, loss: 2.301895\n",
      "Epoch 76, loss: 2.301768\n",
      "Epoch 77, loss: 2.302649\n",
      "Epoch 78, loss: 2.302394\n",
      "Epoch 79, loss: 2.301415\n",
      "Epoch 80, loss: 2.302148\n",
      "Epoch 81, loss: 2.302263\n",
      "Epoch 82, loss: 2.301172\n",
      "Epoch 83, loss: 2.302803\n",
      "Epoch 84, loss: 2.301892\n",
      "Epoch 85, loss: 2.302788\n",
      "Epoch 86, loss: 2.302016\n",
      "Epoch 87, loss: 2.302450\n",
      "Epoch 88, loss: 2.302002\n",
      "Epoch 89, loss: 2.302363\n",
      "Epoch 90, loss: 2.301257\n",
      "Epoch 91, loss: 2.302553\n",
      "Epoch 92, loss: 2.301951\n",
      "Epoch 93, loss: 2.300910\n",
      "Epoch 94, loss: 2.302318\n",
      "Epoch 95, loss: 2.301563\n",
      "Epoch 96, loss: 2.302494\n",
      "Epoch 97, loss: 2.301579\n",
      "Epoch 98, loss: 2.302468\n",
      "Epoch 99, loss: 2.302352\n",
      "Accuracy after training for 100 epochs:  0.121\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "loss = classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.302174\n",
      "Epoch 1, loss: 2.301962\n",
      "Epoch 2, loss: 2.298785\n",
      "Epoch 3, loss: 2.299402\n",
      "Epoch 4, loss: 2.298886\n",
      "Epoch 5, loss: 2.298584\n",
      "Epoch 6, loss: 2.300625\n",
      "Epoch 7, loss: 2.296493\n",
      "Epoch 8, loss: 2.295237\n",
      "Epoch 9, loss: 2.295266\n",
      "Epoch 10, loss: 2.293643\n",
      "Epoch 11, loss: 2.295991\n",
      "Epoch 12, loss: 2.293894\n",
      "Epoch 13, loss: 2.297487\n",
      "Epoch 14, loss: 2.293304\n",
      "Epoch 15, loss: 2.283943\n",
      "Epoch 16, loss: 2.288105\n",
      "Epoch 17, loss: 2.289226\n",
      "Epoch 18, loss: 2.291855\n",
      "Epoch 19, loss: 2.284500\n",
      "Epoch 20, loss: 2.286759\n",
      "Epoch 21, loss: 2.285012\n",
      "Epoch 22, loss: 2.289014\n",
      "Epoch 23, loss: 2.284993\n",
      "Epoch 24, loss: 2.286983\n",
      "Epoch 25, loss: 2.275581\n",
      "Epoch 26, loss: 2.283928\n",
      "Epoch 27, loss: 2.283417\n",
      "Epoch 28, loss: 2.282004\n",
      "Epoch 29, loss: 2.281137\n",
      "Epoch 30, loss: 2.278788\n",
      "Epoch 31, loss: 2.285907\n",
      "Epoch 32, loss: 2.279779\n",
      "Epoch 33, loss: 2.285702\n",
      "Epoch 34, loss: 2.284463\n",
      "Epoch 35, loss: 2.280748\n",
      "Epoch 36, loss: 2.275609\n",
      "Epoch 37, loss: 2.271394\n",
      "Epoch 38, loss: 2.278454\n",
      "Epoch 39, loss: 2.276123\n",
      "Epoch 40, loss: 2.268348\n",
      "Epoch 41, loss: 2.274856\n",
      "Epoch 42, loss: 2.266666\n",
      "Epoch 43, loss: 2.281469\n",
      "Epoch 44, loss: 2.281846\n",
      "Epoch 45, loss: 2.277486\n",
      "Epoch 46, loss: 2.275218\n",
      "Epoch 47, loss: 2.277170\n",
      "Epoch 48, loss: 2.268818\n",
      "Epoch 49, loss: 2.264091\n",
      "Epoch 50, loss: 2.274390\n",
      "Epoch 51, loss: 2.277492\n",
      "Epoch 52, loss: 2.271221\n",
      "Epoch 53, loss: 2.273907\n",
      "Epoch 54, loss: 2.266550\n",
      "Epoch 55, loss: 2.268377\n",
      "Epoch 56, loss: 2.263564\n",
      "Epoch 57, loss: 2.268366\n",
      "Epoch 58, loss: 2.263259\n",
      "Epoch 59, loss: 2.265812\n",
      "Epoch 60, loss: 2.268115\n",
      "Epoch 61, loss: 2.262503\n",
      "Epoch 62, loss: 2.279643\n",
      "Epoch 63, loss: 2.269953\n",
      "Epoch 64, loss: 2.254701\n",
      "Epoch 65, loss: 2.256928\n",
      "Epoch 66, loss: 2.262689\n",
      "Epoch 67, loss: 2.259867\n",
      "Epoch 68, loss: 2.253901\n",
      "Epoch 69, loss: 2.258283\n",
      "Epoch 70, loss: 2.249945\n",
      "Epoch 71, loss: 2.268995\n",
      "Epoch 72, loss: 2.248488\n",
      "Epoch 73, loss: 2.262545\n",
      "Epoch 74, loss: 2.255269\n",
      "Epoch 75, loss: 2.267718\n",
      "Epoch 76, loss: 2.260906\n",
      "Epoch 77, loss: 2.262363\n",
      "Epoch 78, loss: 2.250183\n",
      "Epoch 79, loss: 2.239762\n",
      "Epoch 80, loss: 2.252417\n",
      "Epoch 81, loss: 2.243596\n",
      "Epoch 82, loss: 2.254136\n",
      "Epoch 83, loss: 2.237445\n",
      "Epoch 84, loss: 2.248722\n",
      "Epoch 85, loss: 2.258940\n",
      "Epoch 86, loss: 2.256386\n",
      "Epoch 87, loss: 2.259682\n",
      "Epoch 88, loss: 2.247775\n",
      "Epoch 89, loss: 2.249438\n",
      "Epoch 90, loss: 2.255631\n",
      "Epoch 91, loss: 2.240479\n",
      "Epoch 92, loss: 2.247608\n",
      "Epoch 93, loss: 2.273045\n",
      "Epoch 94, loss: 2.251041\n",
      "Epoch 95, loss: 2.233616\n",
      "Epoch 96, loss: 2.255277\n",
      "Epoch 97, loss: 2.244060\n",
      "Epoch 98, loss: 2.246616\n",
      "Epoch 99, loss: 2.258469\n",
      "Epoch 100, loss: 2.242778\n",
      "Epoch 101, loss: 2.268046\n",
      "Epoch 102, loss: 2.261548\n",
      "Epoch 103, loss: 2.248692\n",
      "Epoch 104, loss: 2.249722\n",
      "Epoch 105, loss: 2.238100\n",
      "Epoch 106, loss: 2.233000\n",
      "Epoch 107, loss: 2.237899\n",
      "Epoch 108, loss: 2.246518\n",
      "Epoch 109, loss: 2.236945\n",
      "Epoch 110, loss: 2.253868\n",
      "Epoch 111, loss: 2.244164\n",
      "Epoch 112, loss: 2.245994\n",
      "Epoch 113, loss: 2.239713\n",
      "Epoch 114, loss: 2.238096\n",
      "Epoch 115, loss: 2.224694\n",
      "Epoch 116, loss: 2.250018\n",
      "Epoch 117, loss: 2.238772\n",
      "Epoch 118, loss: 2.222973\n",
      "Epoch 119, loss: 2.225366\n",
      "Epoch 120, loss: 2.235038\n",
      "Epoch 121, loss: 2.239789\n",
      "Epoch 122, loss: 2.226158\n",
      "Epoch 123, loss: 2.239454\n",
      "Epoch 124, loss: 2.234199\n",
      "Epoch 125, loss: 2.243402\n",
      "Epoch 126, loss: 2.250386\n",
      "Epoch 127, loss: 2.231579\n",
      "Epoch 128, loss: 2.222088\n",
      "Epoch 129, loss: 2.229045\n",
      "Epoch 130, loss: 2.211836\n",
      "Epoch 131, loss: 2.250303\n",
      "Epoch 132, loss: 2.229566\n",
      "Epoch 133, loss: 2.240830\n",
      "Epoch 134, loss: 2.209413\n",
      "Epoch 135, loss: 2.241602\n",
      "Epoch 136, loss: 2.236938\n",
      "Epoch 137, loss: 2.231659\n",
      "Epoch 138, loss: 2.232658\n",
      "Epoch 139, loss: 2.222800\n",
      "Epoch 140, loss: 2.249289\n",
      "Epoch 141, loss: 2.220496\n",
      "Epoch 142, loss: 2.229193\n",
      "Epoch 143, loss: 2.250472\n",
      "Epoch 144, loss: 2.229864\n",
      "Epoch 145, loss: 2.224108\n",
      "Epoch 146, loss: 2.220660\n",
      "Epoch 147, loss: 2.254439\n",
      "Epoch 148, loss: 2.235951\n",
      "Epoch 149, loss: 2.232485\n",
      "Epoch 150, loss: 2.246079\n",
      "Epoch 151, loss: 2.225542\n",
      "Epoch 152, loss: 2.240047\n",
      "Epoch 153, loss: 2.214444\n",
      "Epoch 154, loss: 2.229903\n",
      "Epoch 155, loss: 2.246040\n",
      "Epoch 156, loss: 2.229542\n",
      "Epoch 157, loss: 2.216741\n",
      "Epoch 158, loss: 2.235907\n",
      "Epoch 159, loss: 2.233692\n",
      "Epoch 160, loss: 2.211847\n",
      "Epoch 161, loss: 2.206899\n",
      "Epoch 162, loss: 2.220182\n",
      "Epoch 163, loss: 2.211497\n",
      "Epoch 164, loss: 2.226595\n",
      "Epoch 165, loss: 2.229545\n",
      "Epoch 166, loss: 2.226445\n",
      "Epoch 167, loss: 2.212438\n",
      "Epoch 168, loss: 2.230039\n",
      "Epoch 169, loss: 2.205780\n",
      "Epoch 170, loss: 2.207839\n",
      "Epoch 171, loss: 2.232932\n",
      "Epoch 172, loss: 2.227649\n",
      "Epoch 173, loss: 2.203651\n",
      "Epoch 174, loss: 2.232021\n",
      "Epoch 175, loss: 2.222447\n",
      "Epoch 176, loss: 2.210620\n",
      "Epoch 177, loss: 2.197520\n",
      "Epoch 178, loss: 2.203577\n",
      "Epoch 179, loss: 2.235264\n",
      "Epoch 180, loss: 2.208927\n",
      "Epoch 181, loss: 2.225540\n",
      "Epoch 182, loss: 2.220950\n",
      "Epoch 183, loss: 2.234843\n",
      "Epoch 184, loss: 2.217072\n",
      "Epoch 185, loss: 2.230571\n",
      "Epoch 186, loss: 2.217562\n",
      "Epoch 187, loss: 2.199885\n",
      "Epoch 188, loss: 2.222712\n",
      "Epoch 189, loss: 2.234594\n",
      "Epoch 190, loss: 2.217658\n",
      "Epoch 191, loss: 2.204607\n",
      "Epoch 192, loss: 2.198135\n",
      "Epoch 193, loss: 2.205938\n",
      "Epoch 194, loss: 2.216672\n",
      "Epoch 195, loss: 2.206835\n",
      "Epoch 196, loss: 2.213802\n",
      "Epoch 197, loss: 2.194827\n",
      "Epoch 198, loss: 2.201278\n",
      "Epoch 199, loss: 2.195846\n",
      "Epoch 0, loss: 2.302283\n",
      "Epoch 1, loss: 2.301410\n",
      "Epoch 2, loss: 2.299818\n",
      "Epoch 3, loss: 2.300795\n",
      "Epoch 4, loss: 2.297491\n",
      "Epoch 5, loss: 2.299053\n",
      "Epoch 6, loss: 2.298215\n",
      "Epoch 7, loss: 2.297077\n",
      "Epoch 8, loss: 2.295345\n",
      "Epoch 9, loss: 2.294666\n",
      "Epoch 10, loss: 2.293234\n",
      "Epoch 11, loss: 2.291634\n",
      "Epoch 12, loss: 2.293666\n",
      "Epoch 13, loss: 2.291487\n",
      "Epoch 14, loss: 2.294689\n",
      "Epoch 15, loss: 2.290952\n",
      "Epoch 16, loss: 2.293952\n",
      "Epoch 17, loss: 2.286944\n",
      "Epoch 18, loss: 2.293611\n",
      "Epoch 19, loss: 2.289888\n",
      "Epoch 20, loss: 2.287819\n",
      "Epoch 21, loss: 2.290232\n",
      "Epoch 22, loss: 2.283085\n",
      "Epoch 23, loss: 2.283903\n",
      "Epoch 24, loss: 2.284800\n",
      "Epoch 25, loss: 2.287251\n",
      "Epoch 26, loss: 2.290294\n",
      "Epoch 27, loss: 2.284190\n",
      "Epoch 28, loss: 2.282809\n",
      "Epoch 29, loss: 2.281717\n",
      "Epoch 30, loss: 2.275386\n",
      "Epoch 31, loss: 2.284273\n",
      "Epoch 32, loss: 2.278991\n",
      "Epoch 33, loss: 2.282461\n",
      "Epoch 34, loss: 2.283554\n",
      "Epoch 35, loss: 2.282525\n",
      "Epoch 36, loss: 2.277920\n",
      "Epoch 37, loss: 2.288941\n",
      "Epoch 38, loss: 2.276260\n",
      "Epoch 39, loss: 2.282756\n",
      "Epoch 40, loss: 2.285834\n",
      "Epoch 41, loss: 2.271774\n",
      "Epoch 42, loss: 2.272989\n",
      "Epoch 43, loss: 2.276401\n",
      "Epoch 44, loss: 2.272368\n",
      "Epoch 45, loss: 2.283328\n",
      "Epoch 46, loss: 2.263354\n",
      "Epoch 47, loss: 2.277131\n",
      "Epoch 48, loss: 2.273025\n",
      "Epoch 49, loss: 2.272587\n",
      "Epoch 50, loss: 2.272482\n",
      "Epoch 51, loss: 2.274020\n",
      "Epoch 52, loss: 2.262025\n",
      "Epoch 53, loss: 2.263056\n",
      "Epoch 54, loss: 2.278191\n",
      "Epoch 55, loss: 2.263502\n",
      "Epoch 56, loss: 2.266856\n",
      "Epoch 57, loss: 2.265814\n",
      "Epoch 58, loss: 2.273149\n",
      "Epoch 59, loss: 2.270605\n",
      "Epoch 60, loss: 2.281457\n",
      "Epoch 61, loss: 2.263854\n",
      "Epoch 62, loss: 2.269699\n",
      "Epoch 63, loss: 2.262677\n",
      "Epoch 64, loss: 2.262954\n",
      "Epoch 65, loss: 2.262855\n",
      "Epoch 66, loss: 2.268489\n",
      "Epoch 67, loss: 2.261717\n",
      "Epoch 68, loss: 2.256795\n",
      "Epoch 69, loss: 2.253747\n",
      "Epoch 70, loss: 2.258714\n",
      "Epoch 71, loss: 2.271043\n",
      "Epoch 72, loss: 2.258280\n",
      "Epoch 73, loss: 2.259999\n",
      "Epoch 74, loss: 2.252590\n",
      "Epoch 75, loss: 2.253829\n",
      "Epoch 76, loss: 2.257096\n",
      "Epoch 77, loss: 2.264259\n",
      "Epoch 78, loss: 2.258660\n",
      "Epoch 79, loss: 2.247402\n",
      "Epoch 80, loss: 2.247539\n",
      "Epoch 81, loss: 2.261584\n",
      "Epoch 82, loss: 2.257616\n",
      "Epoch 83, loss: 2.251336\n",
      "Epoch 84, loss: 2.250081\n",
      "Epoch 85, loss: 2.247299\n",
      "Epoch 86, loss: 2.267283\n",
      "Epoch 87, loss: 2.246271\n",
      "Epoch 88, loss: 2.260826\n",
      "Epoch 89, loss: 2.255720\n",
      "Epoch 90, loss: 2.248508\n",
      "Epoch 91, loss: 2.247172\n",
      "Epoch 92, loss: 2.248597\n",
      "Epoch 93, loss: 2.250107\n",
      "Epoch 94, loss: 2.241969\n",
      "Epoch 95, loss: 2.253393\n",
      "Epoch 96, loss: 2.252372\n",
      "Epoch 97, loss: 2.243604\n",
      "Epoch 98, loss: 2.244668\n",
      "Epoch 99, loss: 2.249884\n",
      "Epoch 100, loss: 2.255622\n",
      "Epoch 101, loss: 2.246880\n",
      "Epoch 102, loss: 2.256899\n",
      "Epoch 103, loss: 2.233031\n",
      "Epoch 104, loss: 2.253917\n",
      "Epoch 105, loss: 2.254387\n",
      "Epoch 106, loss: 2.244276\n",
      "Epoch 107, loss: 2.238918\n",
      "Epoch 108, loss: 2.230115\n",
      "Epoch 109, loss: 2.231505\n",
      "Epoch 110, loss: 2.231460\n",
      "Epoch 111, loss: 2.224466\n",
      "Epoch 112, loss: 2.243443\n",
      "Epoch 113, loss: 2.229394\n",
      "Epoch 114, loss: 2.240350\n",
      "Epoch 115, loss: 2.236827\n",
      "Epoch 116, loss: 2.240724\n",
      "Epoch 117, loss: 2.242708\n",
      "Epoch 118, loss: 2.244837\n",
      "Epoch 119, loss: 2.219780\n",
      "Epoch 120, loss: 2.230623\n",
      "Epoch 121, loss: 2.228706\n",
      "Epoch 122, loss: 2.241256\n",
      "Epoch 123, loss: 2.239550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124, loss: 2.259341\n",
      "Epoch 125, loss: 2.236808\n",
      "Epoch 126, loss: 2.226062\n",
      "Epoch 127, loss: 2.226122\n",
      "Epoch 128, loss: 2.234626\n",
      "Epoch 129, loss: 2.217804\n",
      "Epoch 130, loss: 2.247853\n",
      "Epoch 131, loss: 2.249809\n",
      "Epoch 132, loss: 2.229503\n",
      "Epoch 133, loss: 2.224232\n",
      "Epoch 134, loss: 2.233922\n",
      "Epoch 135, loss: 2.253274\n",
      "Epoch 136, loss: 2.236185\n",
      "Epoch 137, loss: 2.233569\n",
      "Epoch 138, loss: 2.239126\n",
      "Epoch 139, loss: 2.234419\n",
      "Epoch 140, loss: 2.215569\n",
      "Epoch 141, loss: 2.235970\n",
      "Epoch 142, loss: 2.217376\n",
      "Epoch 143, loss: 2.238623\n",
      "Epoch 144, loss: 2.225199\n",
      "Epoch 145, loss: 2.245819\n",
      "Epoch 146, loss: 2.211568\n",
      "Epoch 147, loss: 2.212743\n",
      "Epoch 148, loss: 2.204982\n",
      "Epoch 149, loss: 2.247494\n",
      "Epoch 150, loss: 2.249385\n",
      "Epoch 151, loss: 2.240816\n",
      "Epoch 152, loss: 2.228749\n",
      "Epoch 153, loss: 2.227604\n",
      "Epoch 154, loss: 2.224866\n",
      "Epoch 155, loss: 2.239675\n",
      "Epoch 156, loss: 2.228609\n",
      "Epoch 157, loss: 2.206863\n",
      "Epoch 158, loss: 2.248845\n",
      "Epoch 159, loss: 2.246885\n",
      "Epoch 160, loss: 2.228964\n",
      "Epoch 161, loss: 2.216409\n",
      "Epoch 162, loss: 2.230129\n",
      "Epoch 163, loss: 2.225725\n",
      "Epoch 164, loss: 2.222395\n",
      "Epoch 165, loss: 2.220492\n",
      "Epoch 166, loss: 2.203963\n",
      "Epoch 167, loss: 2.215255\n",
      "Epoch 168, loss: 2.233632\n",
      "Epoch 169, loss: 2.232065\n",
      "Epoch 170, loss: 2.216944\n",
      "Epoch 171, loss: 2.235364\n",
      "Epoch 172, loss: 2.213215\n",
      "Epoch 173, loss: 2.194985\n",
      "Epoch 174, loss: 2.244402\n",
      "Epoch 175, loss: 2.234743\n",
      "Epoch 176, loss: 2.215156\n",
      "Epoch 177, loss: 2.213582\n",
      "Epoch 178, loss: 2.211331\n",
      "Epoch 179, loss: 2.225865\n",
      "Epoch 180, loss: 2.224606\n",
      "Epoch 181, loss: 2.199640\n",
      "Epoch 182, loss: 2.224538\n",
      "Epoch 183, loss: 2.210695\n",
      "Epoch 184, loss: 2.225641\n",
      "Epoch 185, loss: 2.226814\n",
      "Epoch 186, loss: 2.196390\n",
      "Epoch 187, loss: 2.208614\n",
      "Epoch 188, loss: 2.238809\n",
      "Epoch 189, loss: 2.219385\n",
      "Epoch 190, loss: 2.231439\n",
      "Epoch 191, loss: 2.228763\n",
      "Epoch 192, loss: 2.226376\n",
      "Epoch 193, loss: 2.214255\n",
      "Epoch 194, loss: 2.224784\n",
      "Epoch 195, loss: 2.214086\n",
      "Epoch 196, loss: 2.240675\n",
      "Epoch 197, loss: 2.181173\n",
      "Epoch 198, loss: 2.223586\n",
      "Epoch 199, loss: 2.198100\n",
      "Epoch 0, loss: 2.302015\n",
      "Epoch 1, loss: 2.302120\n",
      "Epoch 2, loss: 2.299282\n",
      "Epoch 3, loss: 2.300476\n",
      "Epoch 4, loss: 2.299701\n",
      "Epoch 5, loss: 2.298064\n",
      "Epoch 6, loss: 2.298762\n",
      "Epoch 7, loss: 2.297411\n",
      "Epoch 8, loss: 2.295786\n",
      "Epoch 9, loss: 2.296997\n",
      "Epoch 10, loss: 2.294302\n",
      "Epoch 11, loss: 2.293593\n",
      "Epoch 12, loss: 2.297170\n",
      "Epoch 13, loss: 2.293111\n",
      "Epoch 14, loss: 2.290028\n",
      "Epoch 15, loss: 2.296404\n",
      "Epoch 16, loss: 2.291042\n",
      "Epoch 17, loss: 2.290616\n",
      "Epoch 18, loss: 2.290232\n",
      "Epoch 19, loss: 2.292120\n",
      "Epoch 20, loss: 2.283686\n",
      "Epoch 21, loss: 2.285273\n",
      "Epoch 22, loss: 2.291342\n",
      "Epoch 23, loss: 2.286886\n",
      "Epoch 24, loss: 2.280878\n",
      "Epoch 25, loss: 2.281677\n",
      "Epoch 26, loss: 2.281232\n",
      "Epoch 27, loss: 2.281236\n",
      "Epoch 28, loss: 2.284197\n",
      "Epoch 29, loss: 2.282888\n",
      "Epoch 30, loss: 2.282224\n",
      "Epoch 31, loss: 2.277105\n",
      "Epoch 32, loss: 2.284684\n",
      "Epoch 33, loss: 2.269783\n",
      "Epoch 34, loss: 2.275314\n",
      "Epoch 35, loss: 2.279564\n",
      "Epoch 36, loss: 2.276531\n",
      "Epoch 37, loss: 2.271919\n",
      "Epoch 38, loss: 2.282944\n",
      "Epoch 39, loss: 2.274900\n",
      "Epoch 40, loss: 2.278157\n",
      "Epoch 41, loss: 2.276452\n",
      "Epoch 42, loss: 2.278193\n",
      "Epoch 43, loss: 2.272008\n",
      "Epoch 44, loss: 2.277790\n",
      "Epoch 45, loss: 2.267753\n",
      "Epoch 46, loss: 2.267295\n",
      "Epoch 47, loss: 2.274679\n",
      "Epoch 48, loss: 2.270447\n",
      "Epoch 49, loss: 2.269496\n",
      "Epoch 50, loss: 2.266596\n",
      "Epoch 51, loss: 2.263455\n",
      "Epoch 52, loss: 2.271555\n",
      "Epoch 53, loss: 2.277329\n",
      "Epoch 54, loss: 2.258479\n",
      "Epoch 55, loss: 2.259312\n",
      "Epoch 56, loss: 2.262541\n",
      "Epoch 57, loss: 2.270811\n",
      "Epoch 58, loss: 2.271558\n",
      "Epoch 59, loss: 2.265713\n",
      "Epoch 60, loss: 2.273296\n",
      "Epoch 61, loss: 2.276103\n",
      "Epoch 62, loss: 2.264635\n",
      "Epoch 63, loss: 2.267851\n",
      "Epoch 64, loss: 2.260250\n",
      "Epoch 65, loss: 2.273244\n",
      "Epoch 66, loss: 2.260674\n",
      "Epoch 67, loss: 2.261808\n",
      "Epoch 68, loss: 2.267118\n",
      "Epoch 69, loss: 2.262314\n",
      "Epoch 70, loss: 2.257312\n",
      "Epoch 71, loss: 2.258665\n",
      "Epoch 72, loss: 2.252114\n",
      "Epoch 73, loss: 2.251334\n",
      "Epoch 74, loss: 2.265193\n",
      "Epoch 75, loss: 2.252907\n",
      "Epoch 76, loss: 2.255044\n",
      "Epoch 77, loss: 2.262487\n",
      "Epoch 78, loss: 2.252619\n",
      "Epoch 79, loss: 2.261397\n",
      "Epoch 80, loss: 2.260255\n",
      "Epoch 81, loss: 2.246235\n",
      "Epoch 82, loss: 2.248963\n",
      "Epoch 83, loss: 2.255148\n",
      "Epoch 84, loss: 2.251800\n",
      "Epoch 85, loss: 2.254361\n",
      "Epoch 86, loss: 2.249191\n",
      "Epoch 87, loss: 2.243267\n",
      "Epoch 88, loss: 2.261976\n",
      "Epoch 89, loss: 2.248651\n",
      "Epoch 90, loss: 2.233156\n",
      "Epoch 91, loss: 2.258851\n",
      "Epoch 92, loss: 2.247023\n",
      "Epoch 93, loss: 2.252952\n",
      "Epoch 94, loss: 2.245390\n",
      "Epoch 95, loss: 2.247908\n",
      "Epoch 96, loss: 2.246338\n",
      "Epoch 97, loss: 2.243674\n",
      "Epoch 98, loss: 2.246012\n",
      "Epoch 99, loss: 2.229876\n",
      "Epoch 100, loss: 2.251677\n",
      "Epoch 101, loss: 2.244850\n",
      "Epoch 102, loss: 2.245461\n",
      "Epoch 103, loss: 2.235955\n",
      "Epoch 104, loss: 2.249800\n",
      "Epoch 105, loss: 2.246424\n",
      "Epoch 106, loss: 2.252269\n",
      "Epoch 107, loss: 2.229044\n",
      "Epoch 108, loss: 2.256328\n",
      "Epoch 109, loss: 2.227596\n",
      "Epoch 110, loss: 2.231241\n",
      "Epoch 111, loss: 2.232903\n",
      "Epoch 112, loss: 2.241861\n",
      "Epoch 113, loss: 2.255375\n",
      "Epoch 114, loss: 2.244362\n",
      "Epoch 115, loss: 2.249436\n",
      "Epoch 116, loss: 2.232531\n",
      "Epoch 117, loss: 2.254677\n",
      "Epoch 118, loss: 2.234269\n",
      "Epoch 119, loss: 2.229360\n",
      "Epoch 120, loss: 2.248161\n",
      "Epoch 121, loss: 2.234252\n",
      "Epoch 122, loss: 2.254739\n",
      "Epoch 123, loss: 2.237276\n",
      "Epoch 124, loss: 2.243366\n",
      "Epoch 125, loss: 2.240213\n",
      "Epoch 126, loss: 2.213853\n",
      "Epoch 127, loss: 2.228757\n",
      "Epoch 128, loss: 2.206326\n",
      "Epoch 129, loss: 2.237114\n",
      "Epoch 130, loss: 2.222300\n",
      "Epoch 131, loss: 2.215248\n",
      "Epoch 132, loss: 2.226563\n",
      "Epoch 133, loss: 2.226466\n",
      "Epoch 134, loss: 2.245398\n",
      "Epoch 135, loss: 2.219689\n",
      "Epoch 136, loss: 2.226395\n",
      "Epoch 137, loss: 2.238735\n",
      "Epoch 138, loss: 2.243869\n",
      "Epoch 139, loss: 2.229449\n",
      "Epoch 140, loss: 2.223431\n",
      "Epoch 141, loss: 2.226662\n",
      "Epoch 142, loss: 2.229515\n",
      "Epoch 143, loss: 2.240910\n",
      "Epoch 144, loss: 2.235710\n",
      "Epoch 145, loss: 2.216949\n",
      "Epoch 146, loss: 2.238295\n",
      "Epoch 147, loss: 2.238186\n",
      "Epoch 148, loss: 2.223711\n",
      "Epoch 149, loss: 2.237954\n",
      "Epoch 150, loss: 2.235993\n",
      "Epoch 151, loss: 2.214413\n",
      "Epoch 152, loss: 2.226526\n",
      "Epoch 153, loss: 2.223245\n",
      "Epoch 154, loss: 2.215124\n",
      "Epoch 155, loss: 2.220967\n",
      "Epoch 156, loss: 2.204848\n",
      "Epoch 157, loss: 2.201778\n",
      "Epoch 158, loss: 2.217317\n",
      "Epoch 159, loss: 2.212366\n",
      "Epoch 160, loss: 2.242044\n",
      "Epoch 161, loss: 2.232183\n",
      "Epoch 162, loss: 2.234310\n",
      "Epoch 163, loss: 2.209480\n",
      "Epoch 164, loss: 2.244767\n",
      "Epoch 165, loss: 2.210954\n",
      "Epoch 166, loss: 2.207447\n",
      "Epoch 167, loss: 2.209359\n",
      "Epoch 168, loss: 2.232143\n",
      "Epoch 169, loss: 2.218590\n",
      "Epoch 170, loss: 2.204835\n",
      "Epoch 171, loss: 2.230545\n",
      "Epoch 172, loss: 2.229219\n",
      "Epoch 173, loss: 2.228120\n",
      "Epoch 174, loss: 2.221423\n",
      "Epoch 175, loss: 2.226026\n",
      "Epoch 176, loss: 2.224719\n",
      "Epoch 177, loss: 2.232163\n",
      "Epoch 178, loss: 2.211329\n",
      "Epoch 179, loss: 2.229728\n",
      "Epoch 180, loss: 2.184667\n",
      "Epoch 181, loss: 2.221310\n",
      "Epoch 182, loss: 2.209429\n",
      "Epoch 183, loss: 2.218195\n",
      "Epoch 184, loss: 2.234679\n",
      "Epoch 185, loss: 2.211042\n",
      "Epoch 186, loss: 2.230836\n",
      "Epoch 187, loss: 2.215160\n",
      "Epoch 188, loss: 2.221566\n",
      "Epoch 189, loss: 2.224290\n",
      "Epoch 190, loss: 2.215472\n",
      "Epoch 191, loss: 2.227931\n",
      "Epoch 192, loss: 2.212781\n",
      "Epoch 193, loss: 2.211329\n",
      "Epoch 194, loss: 2.204834\n",
      "Epoch 195, loss: 2.217150\n",
      "Epoch 196, loss: 2.209007\n",
      "Epoch 197, loss: 2.217005\n",
      "Epoch 198, loss: 2.219642\n",
      "Epoch 199, loss: 2.215738\n",
      "Epoch 0, loss: 2.302077\n",
      "Epoch 1, loss: 2.298737\n",
      "Epoch 2, loss: 2.299077\n",
      "Epoch 3, loss: 2.302671\n",
      "Epoch 4, loss: 2.295754\n",
      "Epoch 5, loss: 2.289649\n",
      "Epoch 6, loss: 2.297018\n",
      "Epoch 7, loss: 2.290054\n",
      "Epoch 8, loss: 2.292359\n",
      "Epoch 9, loss: 2.284071\n",
      "Epoch 10, loss: 2.288040\n",
      "Epoch 11, loss: 2.286410\n",
      "Epoch 12, loss: 2.281917\n",
      "Epoch 13, loss: 2.283295\n",
      "Epoch 14, loss: 2.291277\n",
      "Epoch 15, loss: 2.281384\n",
      "Epoch 16, loss: 2.277795\n",
      "Epoch 17, loss: 2.278572\n",
      "Epoch 18, loss: 2.284225\n",
      "Epoch 19, loss: 2.279074\n",
      "Epoch 20, loss: 2.257729\n",
      "Epoch 21, loss: 2.269908\n",
      "Epoch 22, loss: 2.279048\n",
      "Epoch 23, loss: 2.278292\n",
      "Epoch 24, loss: 2.262152\n",
      "Epoch 25, loss: 2.272134\n",
      "Epoch 26, loss: 2.270303\n",
      "Epoch 27, loss: 2.263120\n",
      "Epoch 28, loss: 2.273206\n",
      "Epoch 29, loss: 2.275235\n",
      "Epoch 30, loss: 2.261585\n",
      "Epoch 31, loss: 2.262922\n",
      "Epoch 32, loss: 2.271478\n",
      "Epoch 33, loss: 2.262415\n",
      "Epoch 34, loss: 2.256780\n",
      "Epoch 35, loss: 2.261756\n",
      "Epoch 36, loss: 2.257019\n",
      "Epoch 37, loss: 2.264009\n",
      "Epoch 38, loss: 2.255678\n",
      "Epoch 39, loss: 2.260371\n",
      "Epoch 40, loss: 2.250160\n",
      "Epoch 41, loss: 2.260175\n",
      "Epoch 42, loss: 2.268471\n",
      "Epoch 43, loss: 2.235889\n",
      "Epoch 44, loss: 2.246792\n",
      "Epoch 45, loss: 2.249556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, loss: 2.257447\n",
      "Epoch 47, loss: 2.252435\n",
      "Epoch 48, loss: 2.237393\n",
      "Epoch 49, loss: 2.250279\n",
      "Epoch 50, loss: 2.255849\n",
      "Epoch 51, loss: 2.245188\n",
      "Epoch 52, loss: 2.252906\n",
      "Epoch 53, loss: 2.239296\n",
      "Epoch 54, loss: 2.253019\n",
      "Epoch 55, loss: 2.253905\n",
      "Epoch 56, loss: 2.255671\n",
      "Epoch 57, loss: 2.246480\n",
      "Epoch 58, loss: 2.244804\n",
      "Epoch 59, loss: 2.244196\n",
      "Epoch 60, loss: 2.233003\n",
      "Epoch 61, loss: 2.245165\n",
      "Epoch 62, loss: 2.230291\n",
      "Epoch 63, loss: 2.232191\n",
      "Epoch 64, loss: 2.227908\n",
      "Epoch 65, loss: 2.233464\n",
      "Epoch 66, loss: 2.240455\n",
      "Epoch 67, loss: 2.232511\n",
      "Epoch 68, loss: 2.222503\n",
      "Epoch 69, loss: 2.225010\n",
      "Epoch 70, loss: 2.215716\n",
      "Epoch 71, loss: 2.213906\n",
      "Epoch 72, loss: 2.215754\n",
      "Epoch 73, loss: 2.229737\n",
      "Epoch 74, loss: 2.224507\n",
      "Epoch 75, loss: 2.234969\n",
      "Epoch 76, loss: 2.235617\n",
      "Epoch 77, loss: 2.227767\n",
      "Epoch 78, loss: 2.228736\n",
      "Epoch 79, loss: 2.209726\n",
      "Epoch 80, loss: 2.202345\n",
      "Epoch 81, loss: 2.237456\n",
      "Epoch 82, loss: 2.210275\n",
      "Epoch 83, loss: 2.248898\n",
      "Epoch 84, loss: 2.229232\n",
      "Epoch 85, loss: 2.210946\n",
      "Epoch 86, loss: 2.207256\n",
      "Epoch 87, loss: 2.212984\n",
      "Epoch 88, loss: 2.238653\n",
      "Epoch 89, loss: 2.235340\n",
      "Epoch 90, loss: 2.252024\n",
      "Epoch 91, loss: 2.229630\n",
      "Epoch 92, loss: 2.219389\n",
      "Epoch 93, loss: 2.239932\n",
      "Epoch 94, loss: 2.203119\n",
      "Epoch 95, loss: 2.204663\n",
      "Epoch 96, loss: 2.197049\n",
      "Epoch 97, loss: 2.216261\n",
      "Epoch 98, loss: 2.197953\n",
      "Epoch 99, loss: 2.213415\n",
      "Epoch 100, loss: 2.207245\n",
      "Epoch 101, loss: 2.209709\n",
      "Epoch 102, loss: 2.206186\n",
      "Epoch 103, loss: 2.224152\n",
      "Epoch 104, loss: 2.231361\n",
      "Epoch 105, loss: 2.209427\n",
      "Epoch 106, loss: 2.228877\n",
      "Epoch 107, loss: 2.212632\n",
      "Epoch 108, loss: 2.202254\n",
      "Epoch 109, loss: 2.204794\n",
      "Epoch 110, loss: 2.191516\n",
      "Epoch 111, loss: 2.188601\n",
      "Epoch 112, loss: 2.207590\n",
      "Epoch 113, loss: 2.167868\n",
      "Epoch 114, loss: 2.194857\n",
      "Epoch 115, loss: 2.193291\n",
      "Epoch 116, loss: 2.200296\n",
      "Epoch 117, loss: 2.220269\n",
      "Epoch 118, loss: 2.198963\n",
      "Epoch 119, loss: 2.202322\n",
      "Epoch 120, loss: 2.201001\n",
      "Epoch 121, loss: 2.210971\n",
      "Epoch 122, loss: 2.200323\n",
      "Epoch 123, loss: 2.202261\n",
      "Epoch 124, loss: 2.199750\n",
      "Epoch 125, loss: 2.204199\n",
      "Epoch 126, loss: 2.195538\n",
      "Epoch 127, loss: 2.191733\n",
      "Epoch 128, loss: 2.189736\n",
      "Epoch 129, loss: 2.179211\n",
      "Epoch 130, loss: 2.216678\n",
      "Epoch 131, loss: 2.195280\n",
      "Epoch 132, loss: 2.190019\n",
      "Epoch 133, loss: 2.204031\n",
      "Epoch 134, loss: 2.220627\n",
      "Epoch 135, loss: 2.208396\n",
      "Epoch 136, loss: 2.189936\n",
      "Epoch 137, loss: 2.185993\n",
      "Epoch 138, loss: 2.207497\n",
      "Epoch 139, loss: 2.192670\n",
      "Epoch 140, loss: 2.196503\n",
      "Epoch 141, loss: 2.181582\n",
      "Epoch 142, loss: 2.174436\n",
      "Epoch 143, loss: 2.184344\n",
      "Epoch 144, loss: 2.166274\n",
      "Epoch 145, loss: 2.228396\n",
      "Epoch 146, loss: 2.189739\n",
      "Epoch 147, loss: 2.193968\n",
      "Epoch 148, loss: 2.199013\n",
      "Epoch 149, loss: 2.180266\n",
      "Epoch 150, loss: 2.165836\n",
      "Epoch 151, loss: 2.182479\n",
      "Epoch 152, loss: 2.189074\n",
      "Epoch 153, loss: 2.204959\n",
      "Epoch 154, loss: 2.194172\n",
      "Epoch 155, loss: 2.204874\n",
      "Epoch 156, loss: 2.179030\n",
      "Epoch 157, loss: 2.207455\n",
      "Epoch 158, loss: 2.198083\n",
      "Epoch 159, loss: 2.180044\n",
      "Epoch 160, loss: 2.182308\n",
      "Epoch 161, loss: 2.184984\n",
      "Epoch 162, loss: 2.168925\n",
      "Epoch 163, loss: 2.159733\n",
      "Epoch 164, loss: 2.208969\n",
      "Epoch 165, loss: 2.169722\n",
      "Epoch 166, loss: 2.185441\n",
      "Epoch 167, loss: 2.195865\n",
      "Epoch 168, loss: 2.212474\n",
      "Epoch 169, loss: 2.199097\n",
      "Epoch 170, loss: 2.177131\n",
      "Epoch 171, loss: 2.170351\n",
      "Epoch 172, loss: 2.178235\n",
      "Epoch 173, loss: 2.177843\n",
      "Epoch 174, loss: 2.171919\n",
      "Epoch 175, loss: 2.173435\n",
      "Epoch 176, loss: 2.175517\n",
      "Epoch 177, loss: 2.188502\n",
      "Epoch 178, loss: 2.173186\n",
      "Epoch 179, loss: 2.186893\n",
      "Epoch 180, loss: 2.162673\n",
      "Epoch 181, loss: 2.201632\n",
      "Epoch 182, loss: 2.191863\n",
      "Epoch 183, loss: 2.175122\n",
      "Epoch 184, loss: 2.198641\n",
      "Epoch 185, loss: 2.206988\n",
      "Epoch 186, loss: 2.212123\n",
      "Epoch 187, loss: 2.181519\n",
      "Epoch 188, loss: 2.187078\n",
      "Epoch 189, loss: 2.164428\n",
      "Epoch 190, loss: 2.183833\n",
      "Epoch 191, loss: 2.179563\n",
      "Epoch 192, loss: 2.200881\n",
      "Epoch 193, loss: 2.172672\n",
      "Epoch 194, loss: 2.184436\n",
      "Epoch 195, loss: 2.161449\n",
      "Epoch 196, loss: 2.138373\n",
      "Epoch 197, loss: 2.190471\n",
      "Epoch 198, loss: 2.175633\n",
      "Epoch 199, loss: 2.166815\n",
      "Epoch 0, loss: 2.302136\n",
      "Epoch 1, loss: 2.300791\n",
      "Epoch 2, loss: 2.297503\n",
      "Epoch 3, loss: 2.297522\n",
      "Epoch 4, loss: 2.299110\n",
      "Epoch 5, loss: 2.297530\n",
      "Epoch 6, loss: 2.292291\n",
      "Epoch 7, loss: 2.290804\n",
      "Epoch 8, loss: 2.284199\n",
      "Epoch 9, loss: 2.288556\n",
      "Epoch 10, loss: 2.287739\n",
      "Epoch 11, loss: 2.285294\n",
      "Epoch 12, loss: 2.288536\n",
      "Epoch 13, loss: 2.281803\n",
      "Epoch 14, loss: 2.278877\n",
      "Epoch 15, loss: 2.282587\n",
      "Epoch 16, loss: 2.284646\n",
      "Epoch 17, loss: 2.280916\n",
      "Epoch 18, loss: 2.278840\n",
      "Epoch 19, loss: 2.275466\n",
      "Epoch 20, loss: 2.278387\n",
      "Epoch 21, loss: 2.283301\n",
      "Epoch 22, loss: 2.268725\n",
      "Epoch 23, loss: 2.274966\n",
      "Epoch 24, loss: 2.268477\n",
      "Epoch 25, loss: 2.275400\n",
      "Epoch 26, loss: 2.263475\n",
      "Epoch 27, loss: 2.266607\n",
      "Epoch 28, loss: 2.268891\n",
      "Epoch 29, loss: 2.248577\n",
      "Epoch 30, loss: 2.264198\n",
      "Epoch 31, loss: 2.241264\n",
      "Epoch 32, loss: 2.263226\n",
      "Epoch 33, loss: 2.258180\n",
      "Epoch 34, loss: 2.263222\n",
      "Epoch 35, loss: 2.245520\n",
      "Epoch 36, loss: 2.261808\n",
      "Epoch 37, loss: 2.261052\n",
      "Epoch 38, loss: 2.264903\n",
      "Epoch 39, loss: 2.254473\n",
      "Epoch 40, loss: 2.257249\n",
      "Epoch 41, loss: 2.251826\n",
      "Epoch 42, loss: 2.255765\n",
      "Epoch 43, loss: 2.244255\n",
      "Epoch 44, loss: 2.249656\n",
      "Epoch 45, loss: 2.255440\n",
      "Epoch 46, loss: 2.255277\n",
      "Epoch 47, loss: 2.244464\n",
      "Epoch 48, loss: 2.244083\n",
      "Epoch 49, loss: 2.253829\n",
      "Epoch 50, loss: 2.250500\n",
      "Epoch 51, loss: 2.255377\n",
      "Epoch 52, loss: 2.240329\n",
      "Epoch 53, loss: 2.235993\n",
      "Epoch 54, loss: 2.245803\n",
      "Epoch 55, loss: 2.242516\n",
      "Epoch 56, loss: 2.235298\n",
      "Epoch 57, loss: 2.228506\n",
      "Epoch 58, loss: 2.243375\n",
      "Epoch 59, loss: 2.227965\n",
      "Epoch 60, loss: 2.253262\n",
      "Epoch 61, loss: 2.222490\n",
      "Epoch 62, loss: 2.247055\n",
      "Epoch 63, loss: 2.234403\n",
      "Epoch 64, loss: 2.247386\n",
      "Epoch 65, loss: 2.234807\n",
      "Epoch 66, loss: 2.237486\n",
      "Epoch 67, loss: 2.234536\n",
      "Epoch 68, loss: 2.240668\n",
      "Epoch 69, loss: 2.221102\n",
      "Epoch 70, loss: 2.232508\n",
      "Epoch 71, loss: 2.236169\n",
      "Epoch 72, loss: 2.242402\n",
      "Epoch 73, loss: 2.229126\n",
      "Epoch 74, loss: 2.226510\n",
      "Epoch 75, loss: 2.235645\n",
      "Epoch 76, loss: 2.222668\n",
      "Epoch 77, loss: 2.212311\n",
      "Epoch 78, loss: 2.244940\n",
      "Epoch 79, loss: 2.229337\n",
      "Epoch 80, loss: 2.222379\n",
      "Epoch 81, loss: 2.220480\n",
      "Epoch 82, loss: 2.234407\n",
      "Epoch 83, loss: 2.216905\n",
      "Epoch 84, loss: 2.199830\n",
      "Epoch 85, loss: 2.221102\n",
      "Epoch 86, loss: 2.223862\n",
      "Epoch 87, loss: 2.231998\n",
      "Epoch 88, loss: 2.207529\n",
      "Epoch 89, loss: 2.232292\n",
      "Epoch 90, loss: 2.228902\n",
      "Epoch 91, loss: 2.229926\n",
      "Epoch 92, loss: 2.212490\n",
      "Epoch 93, loss: 2.241014\n",
      "Epoch 94, loss: 2.215975\n",
      "Epoch 95, loss: 2.206985\n",
      "Epoch 96, loss: 2.194001\n",
      "Epoch 97, loss: 2.231175\n",
      "Epoch 98, loss: 2.213842\n",
      "Epoch 99, loss: 2.194167\n",
      "Epoch 100, loss: 2.228313\n",
      "Epoch 101, loss: 2.192776\n",
      "Epoch 102, loss: 2.197814\n",
      "Epoch 103, loss: 2.222659\n",
      "Epoch 104, loss: 2.196568\n",
      "Epoch 105, loss: 2.219807\n",
      "Epoch 106, loss: 2.199082\n",
      "Epoch 107, loss: 2.221794\n",
      "Epoch 108, loss: 2.224410\n",
      "Epoch 109, loss: 2.212613\n",
      "Epoch 110, loss: 2.207188\n",
      "Epoch 111, loss: 2.199602\n",
      "Epoch 112, loss: 2.212753\n",
      "Epoch 113, loss: 2.186197\n",
      "Epoch 114, loss: 2.198548\n",
      "Epoch 115, loss: 2.206145\n",
      "Epoch 116, loss: 2.182140\n",
      "Epoch 117, loss: 2.208870\n",
      "Epoch 118, loss: 2.213687\n",
      "Epoch 119, loss: 2.205774\n",
      "Epoch 120, loss: 2.224687\n",
      "Epoch 121, loss: 2.183313\n",
      "Epoch 122, loss: 2.196563\n",
      "Epoch 123, loss: 2.217471\n",
      "Epoch 124, loss: 2.201949\n",
      "Epoch 125, loss: 2.168870\n",
      "Epoch 126, loss: 2.185148\n",
      "Epoch 127, loss: 2.207055\n",
      "Epoch 128, loss: 2.198940\n",
      "Epoch 129, loss: 2.205873\n",
      "Epoch 130, loss: 2.177963\n",
      "Epoch 131, loss: 2.188057\n",
      "Epoch 132, loss: 2.203052\n",
      "Epoch 133, loss: 2.157682\n",
      "Epoch 134, loss: 2.174796\n",
      "Epoch 135, loss: 2.186548\n",
      "Epoch 136, loss: 2.196538\n",
      "Epoch 137, loss: 2.212303\n",
      "Epoch 138, loss: 2.209643\n",
      "Epoch 139, loss: 2.186666\n",
      "Epoch 140, loss: 2.166158\n",
      "Epoch 141, loss: 2.181782\n",
      "Epoch 142, loss: 2.221631\n",
      "Epoch 143, loss: 2.209467\n",
      "Epoch 144, loss: 2.210135\n",
      "Epoch 145, loss: 2.204956\n",
      "Epoch 146, loss: 2.183586\n",
      "Epoch 147, loss: 2.190843\n",
      "Epoch 148, loss: 2.206759\n",
      "Epoch 149, loss: 2.194373\n",
      "Epoch 150, loss: 2.197673\n",
      "Epoch 151, loss: 2.188691\n",
      "Epoch 152, loss: 2.201412\n",
      "Epoch 153, loss: 2.176206\n",
      "Epoch 154, loss: 2.202146\n",
      "Epoch 155, loss: 2.186840\n",
      "Epoch 156, loss: 2.193870\n",
      "Epoch 157, loss: 2.174561\n",
      "Epoch 158, loss: 2.197360\n",
      "Epoch 159, loss: 2.210294\n",
      "Epoch 160, loss: 2.189709\n",
      "Epoch 161, loss: 2.173769\n",
      "Epoch 162, loss: 2.170971\n",
      "Epoch 163, loss: 2.192298\n",
      "Epoch 164, loss: 2.182716\n",
      "Epoch 165, loss: 2.205323\n",
      "Epoch 166, loss: 2.194480\n",
      "Epoch 167, loss: 2.182487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168, loss: 2.194970\n",
      "Epoch 169, loss: 2.150602\n",
      "Epoch 170, loss: 2.172743\n",
      "Epoch 171, loss: 2.186625\n",
      "Epoch 172, loss: 2.167030\n",
      "Epoch 173, loss: 2.183897\n",
      "Epoch 174, loss: 2.206264\n",
      "Epoch 175, loss: 2.172475\n",
      "Epoch 176, loss: 2.163465\n",
      "Epoch 177, loss: 2.197021\n",
      "Epoch 178, loss: 2.203084\n",
      "Epoch 179, loss: 2.206213\n",
      "Epoch 180, loss: 2.203506\n",
      "Epoch 181, loss: 2.136452\n",
      "Epoch 182, loss: 2.187330\n",
      "Epoch 183, loss: 2.143044\n",
      "Epoch 184, loss: 2.195669\n",
      "Epoch 185, loss: 2.195119\n",
      "Epoch 186, loss: 2.184709\n",
      "Epoch 187, loss: 2.201659\n",
      "Epoch 188, loss: 2.203633\n",
      "Epoch 189, loss: 2.183325\n",
      "Epoch 190, loss: 2.159601\n",
      "Epoch 191, loss: 2.180400\n",
      "Epoch 192, loss: 2.167735\n",
      "Epoch 193, loss: 2.169000\n",
      "Epoch 194, loss: 2.170323\n",
      "Epoch 195, loss: 2.174844\n",
      "Epoch 196, loss: 2.168037\n",
      "Epoch 197, loss: 2.118253\n",
      "Epoch 198, loss: 2.175090\n",
      "Epoch 199, loss: 2.186023\n",
      "Epoch 0, loss: 2.300973\n",
      "Epoch 1, loss: 2.299070\n",
      "Epoch 2, loss: 2.302117\n",
      "Epoch 3, loss: 2.296424\n",
      "Epoch 4, loss: 2.296984\n",
      "Epoch 5, loss: 2.293203\n",
      "Epoch 6, loss: 2.299142\n",
      "Epoch 7, loss: 2.290245\n",
      "Epoch 8, loss: 2.292569\n",
      "Epoch 9, loss: 2.291467\n",
      "Epoch 10, loss: 2.291258\n",
      "Epoch 11, loss: 2.286286\n",
      "Epoch 12, loss: 2.292889\n",
      "Epoch 13, loss: 2.281138\n",
      "Epoch 14, loss: 2.270782\n",
      "Epoch 15, loss: 2.283204\n",
      "Epoch 16, loss: 2.278041\n",
      "Epoch 17, loss: 2.280308\n",
      "Epoch 18, loss: 2.274829\n",
      "Epoch 19, loss: 2.274200\n",
      "Epoch 20, loss: 2.280680\n",
      "Epoch 21, loss: 2.262142\n",
      "Epoch 22, loss: 2.275355\n",
      "Epoch 23, loss: 2.273728\n",
      "Epoch 24, loss: 2.272979\n",
      "Epoch 25, loss: 2.275024\n",
      "Epoch 26, loss: 2.271736\n",
      "Epoch 27, loss: 2.271175\n",
      "Epoch 28, loss: 2.262991\n",
      "Epoch 29, loss: 2.270170\n",
      "Epoch 30, loss: 2.258605\n",
      "Epoch 31, loss: 2.261905\n",
      "Epoch 32, loss: 2.265511\n",
      "Epoch 33, loss: 2.255740\n",
      "Epoch 34, loss: 2.262935\n",
      "Epoch 35, loss: 2.256604\n",
      "Epoch 36, loss: 2.257141\n",
      "Epoch 37, loss: 2.256643\n",
      "Epoch 38, loss: 2.265654\n",
      "Epoch 39, loss: 2.264607\n",
      "Epoch 40, loss: 2.257643\n",
      "Epoch 41, loss: 2.260387\n",
      "Epoch 42, loss: 2.248436\n",
      "Epoch 43, loss: 2.251286\n",
      "Epoch 44, loss: 2.254591\n",
      "Epoch 45, loss: 2.242863\n",
      "Epoch 46, loss: 2.245906\n",
      "Epoch 47, loss: 2.253285\n",
      "Epoch 48, loss: 2.244576\n",
      "Epoch 49, loss: 2.243171\n",
      "Epoch 50, loss: 2.241878\n",
      "Epoch 51, loss: 2.264114\n",
      "Epoch 52, loss: 2.244593\n",
      "Epoch 53, loss: 2.241896\n",
      "Epoch 54, loss: 2.246664\n",
      "Epoch 55, loss: 2.240326\n",
      "Epoch 56, loss: 2.230985\n",
      "Epoch 57, loss: 2.236887\n",
      "Epoch 58, loss: 2.234239\n",
      "Epoch 59, loss: 2.228496\n",
      "Epoch 60, loss: 2.236303\n",
      "Epoch 61, loss: 2.240177\n",
      "Epoch 62, loss: 2.237638\n",
      "Epoch 63, loss: 2.230391\n",
      "Epoch 64, loss: 2.224145\n",
      "Epoch 65, loss: 2.237889\n",
      "Epoch 66, loss: 2.223148\n",
      "Epoch 67, loss: 2.220842\n",
      "Epoch 68, loss: 2.222948\n",
      "Epoch 69, loss: 2.229826\n",
      "Epoch 70, loss: 2.212293\n",
      "Epoch 71, loss: 2.247359\n",
      "Epoch 72, loss: 2.244975\n",
      "Epoch 73, loss: 2.241685\n",
      "Epoch 74, loss: 2.242066\n",
      "Epoch 75, loss: 2.244988\n",
      "Epoch 76, loss: 2.230029\n",
      "Epoch 77, loss: 2.215638\n",
      "Epoch 78, loss: 2.209575\n",
      "Epoch 79, loss: 2.223759\n",
      "Epoch 80, loss: 2.241269\n",
      "Epoch 81, loss: 2.192285\n",
      "Epoch 82, loss: 2.214554\n",
      "Epoch 83, loss: 2.229824\n",
      "Epoch 84, loss: 2.229443\n",
      "Epoch 85, loss: 2.228558\n",
      "Epoch 86, loss: 2.212901\n",
      "Epoch 87, loss: 2.214615\n",
      "Epoch 88, loss: 2.223649\n",
      "Epoch 89, loss: 2.235121\n",
      "Epoch 90, loss: 2.245861\n",
      "Epoch 91, loss: 2.205738\n",
      "Epoch 92, loss: 2.212772\n",
      "Epoch 93, loss: 2.221199\n",
      "Epoch 94, loss: 2.219726\n",
      "Epoch 95, loss: 2.214587\n",
      "Epoch 96, loss: 2.206422\n",
      "Epoch 97, loss: 2.235713\n",
      "Epoch 98, loss: 2.213389\n",
      "Epoch 99, loss: 2.225034\n",
      "Epoch 100, loss: 2.218543\n",
      "Epoch 101, loss: 2.191931\n",
      "Epoch 102, loss: 2.207820\n",
      "Epoch 103, loss: 2.216976\n",
      "Epoch 104, loss: 2.198971\n",
      "Epoch 105, loss: 2.210105\n",
      "Epoch 106, loss: 2.220263\n",
      "Epoch 107, loss: 2.205894\n",
      "Epoch 108, loss: 2.203454\n",
      "Epoch 109, loss: 2.168670\n",
      "Epoch 110, loss: 2.197775\n",
      "Epoch 111, loss: 2.216730\n",
      "Epoch 112, loss: 2.225099\n",
      "Epoch 113, loss: 2.185084\n",
      "Epoch 114, loss: 2.175369\n",
      "Epoch 115, loss: 2.188021\n",
      "Epoch 116, loss: 2.210630\n",
      "Epoch 117, loss: 2.220840\n",
      "Epoch 118, loss: 2.214166\n",
      "Epoch 119, loss: 2.177139\n",
      "Epoch 120, loss: 2.185925\n",
      "Epoch 121, loss: 2.204989\n",
      "Epoch 122, loss: 2.217456\n",
      "Epoch 123, loss: 2.177427\n",
      "Epoch 124, loss: 2.205882\n",
      "Epoch 125, loss: 2.189961\n",
      "Epoch 126, loss: 2.208798\n",
      "Epoch 127, loss: 2.203716\n",
      "Epoch 128, loss: 2.183376\n",
      "Epoch 129, loss: 2.221946\n",
      "Epoch 130, loss: 2.203595\n",
      "Epoch 131, loss: 2.179850\n",
      "Epoch 132, loss: 2.193888\n",
      "Epoch 133, loss: 2.207591\n",
      "Epoch 134, loss: 2.178517\n",
      "Epoch 135, loss: 2.199400\n",
      "Epoch 136, loss: 2.204269\n",
      "Epoch 137, loss: 2.191812\n",
      "Epoch 138, loss: 2.172776\n",
      "Epoch 139, loss: 2.200774\n",
      "Epoch 140, loss: 2.184155\n",
      "Epoch 141, loss: 2.173618\n",
      "Epoch 142, loss: 2.193747\n",
      "Epoch 143, loss: 2.190099\n",
      "Epoch 144, loss: 2.187488\n",
      "Epoch 145, loss: 2.213679\n",
      "Epoch 146, loss: 2.174842\n",
      "Epoch 147, loss: 2.204774\n",
      "Epoch 148, loss: 2.187956\n",
      "Epoch 149, loss: 2.224158\n",
      "Epoch 150, loss: 2.197834\n",
      "Epoch 151, loss: 2.207762\n",
      "Epoch 152, loss: 2.178099\n",
      "Epoch 153, loss: 2.182750\n",
      "Epoch 154, loss: 2.219852\n",
      "Epoch 155, loss: 2.220597\n",
      "Epoch 156, loss: 2.195121\n",
      "Epoch 157, loss: 2.169466\n",
      "Epoch 158, loss: 2.201442\n",
      "Epoch 159, loss: 2.193513\n",
      "Epoch 160, loss: 2.147639\n",
      "Epoch 161, loss: 2.164748\n",
      "Epoch 162, loss: 2.236280\n",
      "Epoch 163, loss: 2.173801\n",
      "Epoch 164, loss: 2.177540\n",
      "Epoch 165, loss: 2.142505\n",
      "Epoch 166, loss: 2.162725\n",
      "Epoch 167, loss: 2.157948\n",
      "Epoch 168, loss: 2.223267\n",
      "Epoch 169, loss: 2.182172\n",
      "Epoch 170, loss: 2.158626\n",
      "Epoch 171, loss: 2.182680\n",
      "Epoch 172, loss: 2.156845\n",
      "Epoch 173, loss: 2.158115\n",
      "Epoch 174, loss: 2.204659\n",
      "Epoch 175, loss: 2.154387\n",
      "Epoch 176, loss: 2.164107\n",
      "Epoch 177, loss: 2.177551\n",
      "Epoch 178, loss: 2.171858\n",
      "Epoch 179, loss: 2.177790\n",
      "Epoch 180, loss: 2.160526\n",
      "Epoch 181, loss: 2.153277\n",
      "Epoch 182, loss: 2.167807\n",
      "Epoch 183, loss: 2.202378\n",
      "Epoch 184, loss: 2.213063\n",
      "Epoch 185, loss: 2.198860\n",
      "Epoch 186, loss: 2.193596\n",
      "Epoch 187, loss: 2.159821\n",
      "Epoch 188, loss: 2.229519\n",
      "Epoch 189, loss: 2.175998\n",
      "Epoch 190, loss: 2.176766\n",
      "Epoch 191, loss: 2.172595\n",
      "Epoch 192, loss: 2.170616\n",
      "Epoch 193, loss: 2.178221\n",
      "Epoch 194, loss: 2.181200\n",
      "Epoch 195, loss: 2.115602\n",
      "Epoch 196, loss: 2.191822\n",
      "Epoch 197, loss: 2.155176\n",
      "Epoch 198, loss: 2.166506\n",
      "Epoch 199, loss: 2.204148\n",
      "Epoch 0, loss: 2.301125\n",
      "Epoch 1, loss: 2.302200\n",
      "Epoch 2, loss: 2.301742\n",
      "Epoch 3, loss: 2.302093\n",
      "Epoch 4, loss: 2.301926\n",
      "Epoch 5, loss: 2.302497\n",
      "Epoch 6, loss: 2.302170\n",
      "Epoch 7, loss: 2.301577\n",
      "Epoch 8, loss: 2.301461\n",
      "Epoch 9, loss: 2.302619\n",
      "Epoch 10, loss: 2.300737\n",
      "Epoch 11, loss: 2.302274\n",
      "Epoch 12, loss: 2.300616\n",
      "Epoch 13, loss: 2.301745\n",
      "Epoch 14, loss: 2.302550\n",
      "Epoch 15, loss: 2.301103\n",
      "Epoch 16, loss: 2.302168\n",
      "Epoch 17, loss: 2.300112\n",
      "Epoch 18, loss: 2.300364\n",
      "Epoch 19, loss: 2.301893\n",
      "Epoch 20, loss: 2.300337\n",
      "Epoch 21, loss: 2.300054\n",
      "Epoch 22, loss: 2.300775\n",
      "Epoch 23, loss: 2.300024\n",
      "Epoch 24, loss: 2.300854\n",
      "Epoch 25, loss: 2.302759\n",
      "Epoch 26, loss: 2.299918\n",
      "Epoch 27, loss: 2.299493\n",
      "Epoch 28, loss: 2.300792\n",
      "Epoch 29, loss: 2.301022\n",
      "Epoch 30, loss: 2.300825\n",
      "Epoch 31, loss: 2.300088\n",
      "Epoch 32, loss: 2.301286\n",
      "Epoch 33, loss: 2.299226\n",
      "Epoch 34, loss: 2.301055\n",
      "Epoch 35, loss: 2.298396\n",
      "Epoch 36, loss: 2.300294\n",
      "Epoch 37, loss: 2.298941\n",
      "Epoch 38, loss: 2.300911\n",
      "Epoch 39, loss: 2.296968\n",
      "Epoch 40, loss: 2.301011\n",
      "Epoch 41, loss: 2.297449\n",
      "Epoch 42, loss: 2.297345\n",
      "Epoch 43, loss: 2.300734\n",
      "Epoch 44, loss: 2.299636\n",
      "Epoch 45, loss: 2.298815\n",
      "Epoch 46, loss: 2.297777\n",
      "Epoch 47, loss: 2.297139\n",
      "Epoch 48, loss: 2.297954\n",
      "Epoch 49, loss: 2.299016\n",
      "Epoch 50, loss: 2.299166\n",
      "Epoch 51, loss: 2.298032\n",
      "Epoch 52, loss: 2.299755\n",
      "Epoch 53, loss: 2.297045\n",
      "Epoch 54, loss: 2.298063\n",
      "Epoch 55, loss: 2.297764\n",
      "Epoch 56, loss: 2.296007\n",
      "Epoch 57, loss: 2.297354\n",
      "Epoch 58, loss: 2.300478\n",
      "Epoch 59, loss: 2.296845\n",
      "Epoch 60, loss: 2.294570\n",
      "Epoch 61, loss: 2.301225\n",
      "Epoch 62, loss: 2.296991\n",
      "Epoch 63, loss: 2.297347\n",
      "Epoch 64, loss: 2.299414\n",
      "Epoch 65, loss: 2.297538\n",
      "Epoch 66, loss: 2.297877\n",
      "Epoch 67, loss: 2.298067\n",
      "Epoch 68, loss: 2.298602\n",
      "Epoch 69, loss: 2.297808\n",
      "Epoch 70, loss: 2.297326\n",
      "Epoch 71, loss: 2.296027\n",
      "Epoch 72, loss: 2.297264\n",
      "Epoch 73, loss: 2.298253\n",
      "Epoch 74, loss: 2.297081\n",
      "Epoch 75, loss: 2.296681\n",
      "Epoch 76, loss: 2.295135\n",
      "Epoch 77, loss: 2.296484\n",
      "Epoch 78, loss: 2.293180\n",
      "Epoch 79, loss: 2.294982\n",
      "Epoch 80, loss: 2.297377\n",
      "Epoch 81, loss: 2.300328\n",
      "Epoch 82, loss: 2.296011\n",
      "Epoch 83, loss: 2.296407\n",
      "Epoch 84, loss: 2.292792\n",
      "Epoch 85, loss: 2.295607\n",
      "Epoch 86, loss: 2.295770\n",
      "Epoch 87, loss: 2.294069\n",
      "Epoch 88, loss: 2.295654\n",
      "Epoch 89, loss: 2.291652\n",
      "Epoch 90, loss: 2.293328\n",
      "Epoch 91, loss: 2.296209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, loss: 2.293167\n",
      "Epoch 93, loss: 2.293064\n",
      "Epoch 94, loss: 2.294713\n",
      "Epoch 95, loss: 2.292432\n",
      "Epoch 96, loss: 2.296000\n",
      "Epoch 97, loss: 2.297332\n",
      "Epoch 98, loss: 2.293959\n",
      "Epoch 99, loss: 2.295950\n",
      "Epoch 100, loss: 2.295298\n",
      "Epoch 101, loss: 2.295924\n",
      "Epoch 102, loss: 2.297352\n",
      "Epoch 103, loss: 2.295070\n",
      "Epoch 104, loss: 2.295764\n",
      "Epoch 105, loss: 2.297860\n",
      "Epoch 106, loss: 2.294103\n",
      "Epoch 107, loss: 2.294190\n",
      "Epoch 108, loss: 2.293610\n",
      "Epoch 109, loss: 2.295236\n",
      "Epoch 110, loss: 2.294912\n",
      "Epoch 111, loss: 2.292320\n",
      "Epoch 112, loss: 2.290753\n",
      "Epoch 113, loss: 2.294179\n",
      "Epoch 114, loss: 2.293030\n",
      "Epoch 115, loss: 2.293399\n",
      "Epoch 116, loss: 2.295639\n",
      "Epoch 117, loss: 2.290778\n",
      "Epoch 118, loss: 2.295306\n",
      "Epoch 119, loss: 2.296104\n",
      "Epoch 120, loss: 2.295446\n",
      "Epoch 121, loss: 2.290728\n",
      "Epoch 122, loss: 2.294791\n",
      "Epoch 123, loss: 2.293126\n",
      "Epoch 124, loss: 2.297149\n",
      "Epoch 125, loss: 2.293764\n",
      "Epoch 126, loss: 2.293829\n",
      "Epoch 127, loss: 2.298835\n",
      "Epoch 128, loss: 2.297003\n",
      "Epoch 129, loss: 2.290468\n",
      "Epoch 130, loss: 2.291062\n",
      "Epoch 131, loss: 2.291445\n",
      "Epoch 132, loss: 2.292736\n",
      "Epoch 133, loss: 2.292861\n",
      "Epoch 134, loss: 2.289004\n",
      "Epoch 135, loss: 2.290572\n",
      "Epoch 136, loss: 2.294483\n",
      "Epoch 137, loss: 2.291089\n",
      "Epoch 138, loss: 2.295021\n",
      "Epoch 139, loss: 2.287996\n",
      "Epoch 140, loss: 2.291653\n",
      "Epoch 141, loss: 2.295224\n",
      "Epoch 142, loss: 2.294544\n",
      "Epoch 143, loss: 2.291490\n",
      "Epoch 144, loss: 2.290732\n",
      "Epoch 145, loss: 2.290903\n",
      "Epoch 146, loss: 2.293225\n",
      "Epoch 147, loss: 2.288393\n",
      "Epoch 148, loss: 2.288340\n",
      "Epoch 149, loss: 2.290701\n",
      "Epoch 150, loss: 2.291807\n",
      "Epoch 151, loss: 2.289058\n",
      "Epoch 152, loss: 2.291494\n",
      "Epoch 153, loss: 2.290863\n",
      "Epoch 154, loss: 2.291588\n",
      "Epoch 155, loss: 2.293843\n",
      "Epoch 156, loss: 2.288540\n",
      "Epoch 157, loss: 2.293105\n",
      "Epoch 158, loss: 2.290755\n",
      "Epoch 159, loss: 2.290011\n",
      "Epoch 160, loss: 2.290204\n",
      "Epoch 161, loss: 2.294789\n",
      "Epoch 162, loss: 2.288955\n",
      "Epoch 163, loss: 2.287953\n",
      "Epoch 164, loss: 2.290451\n",
      "Epoch 165, loss: 2.288906\n",
      "Epoch 166, loss: 2.290077\n",
      "Epoch 167, loss: 2.292534\n",
      "Epoch 168, loss: 2.289904\n",
      "Epoch 169, loss: 2.293834\n",
      "Epoch 170, loss: 2.290196\n",
      "Epoch 171, loss: 2.292091\n",
      "Epoch 172, loss: 2.286254\n",
      "Epoch 173, loss: 2.287391\n",
      "Epoch 174, loss: 2.286621\n",
      "Epoch 175, loss: 2.292097\n",
      "Epoch 176, loss: 2.288507\n",
      "Epoch 177, loss: 2.290719\n",
      "Epoch 178, loss: 2.291524\n",
      "Epoch 179, loss: 2.290631\n",
      "Epoch 180, loss: 2.292892\n",
      "Epoch 181, loss: 2.290845\n",
      "Epoch 182, loss: 2.284988\n",
      "Epoch 183, loss: 2.285761\n",
      "Epoch 184, loss: 2.290234\n",
      "Epoch 185, loss: 2.290158\n",
      "Epoch 186, loss: 2.285782\n",
      "Epoch 187, loss: 2.286718\n",
      "Epoch 188, loss: 2.292804\n",
      "Epoch 189, loss: 2.289484\n",
      "Epoch 190, loss: 2.290688\n",
      "Epoch 191, loss: 2.291084\n",
      "Epoch 192, loss: 2.286525\n",
      "Epoch 193, loss: 2.293068\n",
      "Epoch 194, loss: 2.289601\n",
      "Epoch 195, loss: 2.286911\n",
      "Epoch 196, loss: 2.279879\n",
      "Epoch 197, loss: 2.292345\n",
      "Epoch 198, loss: 2.291358\n",
      "Epoch 199, loss: 2.290731\n",
      "Epoch 0, loss: 2.303110\n",
      "Epoch 1, loss: 2.302142\n",
      "Epoch 2, loss: 2.302105\n",
      "Epoch 3, loss: 2.302129\n",
      "Epoch 4, loss: 2.301871\n",
      "Epoch 5, loss: 2.301514\n",
      "Epoch 6, loss: 2.301108\n",
      "Epoch 7, loss: 2.301050\n",
      "Epoch 8, loss: 2.302568\n",
      "Epoch 9, loss: 2.301792\n",
      "Epoch 10, loss: 2.301391\n",
      "Epoch 11, loss: 2.301077\n",
      "Epoch 12, loss: 2.301629\n",
      "Epoch 13, loss: 2.301287\n",
      "Epoch 14, loss: 2.302667\n",
      "Epoch 15, loss: 2.300957\n",
      "Epoch 16, loss: 2.299444\n",
      "Epoch 17, loss: 2.300896\n",
      "Epoch 18, loss: 2.300120\n",
      "Epoch 19, loss: 2.301182\n",
      "Epoch 20, loss: 2.300271\n",
      "Epoch 21, loss: 2.300147\n",
      "Epoch 22, loss: 2.301258\n",
      "Epoch 23, loss: 2.299016\n",
      "Epoch 24, loss: 2.298317\n",
      "Epoch 25, loss: 2.300794\n",
      "Epoch 26, loss: 2.298622\n",
      "Epoch 27, loss: 2.299880\n",
      "Epoch 28, loss: 2.297454\n",
      "Epoch 29, loss: 2.298518\n",
      "Epoch 30, loss: 2.298975\n",
      "Epoch 31, loss: 2.300696\n",
      "Epoch 32, loss: 2.299317\n",
      "Epoch 33, loss: 2.298052\n",
      "Epoch 34, loss: 2.297766\n",
      "Epoch 35, loss: 2.302060\n",
      "Epoch 36, loss: 2.298698\n",
      "Epoch 37, loss: 2.298955\n",
      "Epoch 38, loss: 2.297930\n",
      "Epoch 39, loss: 2.298327\n",
      "Epoch 40, loss: 2.300036\n",
      "Epoch 41, loss: 2.302148\n",
      "Epoch 42, loss: 2.299310\n",
      "Epoch 43, loss: 2.297714\n",
      "Epoch 44, loss: 2.300002\n",
      "Epoch 45, loss: 2.297268\n",
      "Epoch 46, loss: 2.298092\n",
      "Epoch 47, loss: 2.295271\n",
      "Epoch 48, loss: 2.298917\n",
      "Epoch 49, loss: 2.297884\n",
      "Epoch 50, loss: 2.299312\n",
      "Epoch 51, loss: 2.298073\n",
      "Epoch 52, loss: 2.297490\n",
      "Epoch 53, loss: 2.298897\n",
      "Epoch 54, loss: 2.297314\n",
      "Epoch 55, loss: 2.296932\n",
      "Epoch 56, loss: 2.296719\n",
      "Epoch 57, loss: 2.296454\n",
      "Epoch 58, loss: 2.297486\n",
      "Epoch 59, loss: 2.296463\n",
      "Epoch 60, loss: 2.294956\n",
      "Epoch 61, loss: 2.294303\n",
      "Epoch 62, loss: 2.295693\n",
      "Epoch 63, loss: 2.295944\n",
      "Epoch 64, loss: 2.296983\n",
      "Epoch 65, loss: 2.293753\n",
      "Epoch 66, loss: 2.297513\n",
      "Epoch 67, loss: 2.297635\n",
      "Epoch 68, loss: 2.297279\n",
      "Epoch 69, loss: 2.295288\n",
      "Epoch 70, loss: 2.296705\n",
      "Epoch 71, loss: 2.296090\n",
      "Epoch 72, loss: 2.294790\n",
      "Epoch 73, loss: 2.297833\n",
      "Epoch 74, loss: 2.297703\n",
      "Epoch 75, loss: 2.296245\n",
      "Epoch 76, loss: 2.296422\n",
      "Epoch 77, loss: 2.298882\n",
      "Epoch 78, loss: 2.296739\n",
      "Epoch 79, loss: 2.295119\n",
      "Epoch 80, loss: 2.298557\n",
      "Epoch 81, loss: 2.296233\n",
      "Epoch 82, loss: 2.298064\n",
      "Epoch 83, loss: 2.299205\n",
      "Epoch 84, loss: 2.295602\n",
      "Epoch 85, loss: 2.292912\n",
      "Epoch 86, loss: 2.293413\n",
      "Epoch 87, loss: 2.293288\n",
      "Epoch 88, loss: 2.293577\n",
      "Epoch 89, loss: 2.298187\n",
      "Epoch 90, loss: 2.292380\n",
      "Epoch 91, loss: 2.294156\n",
      "Epoch 92, loss: 2.295353\n",
      "Epoch 93, loss: 2.295348\n",
      "Epoch 94, loss: 2.293960\n",
      "Epoch 95, loss: 2.295478\n",
      "Epoch 96, loss: 2.299529\n",
      "Epoch 97, loss: 2.297862\n",
      "Epoch 98, loss: 2.294988\n",
      "Epoch 99, loss: 2.291628\n",
      "Epoch 100, loss: 2.293605\n",
      "Epoch 101, loss: 2.296441\n",
      "Epoch 102, loss: 2.294089\n",
      "Epoch 103, loss: 2.289441\n",
      "Epoch 104, loss: 2.294790\n",
      "Epoch 105, loss: 2.295374\n",
      "Epoch 106, loss: 2.293140\n",
      "Epoch 107, loss: 2.293432\n",
      "Epoch 108, loss: 2.293446\n",
      "Epoch 109, loss: 2.292786\n",
      "Epoch 110, loss: 2.295249\n",
      "Epoch 111, loss: 2.293797\n",
      "Epoch 112, loss: 2.294295\n",
      "Epoch 113, loss: 2.295866\n",
      "Epoch 114, loss: 2.291188\n",
      "Epoch 115, loss: 2.293878\n",
      "Epoch 116, loss: 2.295311\n",
      "Epoch 117, loss: 2.292040\n",
      "Epoch 118, loss: 2.295451\n",
      "Epoch 119, loss: 2.294980\n",
      "Epoch 120, loss: 2.298338\n",
      "Epoch 121, loss: 2.295627\n",
      "Epoch 122, loss: 2.292127\n",
      "Epoch 123, loss: 2.289816\n",
      "Epoch 124, loss: 2.292666\n",
      "Epoch 125, loss: 2.293806\n",
      "Epoch 126, loss: 2.291567\n",
      "Epoch 127, loss: 2.293051\n",
      "Epoch 128, loss: 2.292135\n",
      "Epoch 129, loss: 2.293590\n",
      "Epoch 130, loss: 2.292603\n",
      "Epoch 131, loss: 2.295385\n",
      "Epoch 132, loss: 2.292609\n",
      "Epoch 133, loss: 2.294717\n",
      "Epoch 134, loss: 2.294911\n",
      "Epoch 135, loss: 2.293175\n",
      "Epoch 136, loss: 2.292332\n",
      "Epoch 137, loss: 2.291382\n",
      "Epoch 138, loss: 2.292153\n",
      "Epoch 139, loss: 2.288385\n",
      "Epoch 140, loss: 2.293056\n",
      "Epoch 141, loss: 2.289768\n",
      "Epoch 142, loss: 2.293371\n",
      "Epoch 143, loss: 2.288057\n",
      "Epoch 144, loss: 2.289265\n",
      "Epoch 145, loss: 2.291239\n",
      "Epoch 146, loss: 2.288643\n",
      "Epoch 147, loss: 2.292158\n",
      "Epoch 148, loss: 2.289275\n",
      "Epoch 149, loss: 2.291586\n",
      "Epoch 150, loss: 2.292307\n",
      "Epoch 151, loss: 2.291459\n",
      "Epoch 152, loss: 2.291573\n",
      "Epoch 153, loss: 2.292009\n",
      "Epoch 154, loss: 2.288856\n",
      "Epoch 155, loss: 2.294451\n",
      "Epoch 156, loss: 2.293629\n",
      "Epoch 157, loss: 2.291818\n",
      "Epoch 158, loss: 2.292578\n",
      "Epoch 159, loss: 2.288502\n",
      "Epoch 160, loss: 2.287575\n",
      "Epoch 161, loss: 2.289591\n",
      "Epoch 162, loss: 2.297444\n",
      "Epoch 163, loss: 2.288685\n",
      "Epoch 164, loss: 2.290328\n",
      "Epoch 165, loss: 2.289374\n",
      "Epoch 166, loss: 2.288450\n",
      "Epoch 167, loss: 2.291275\n",
      "Epoch 168, loss: 2.289609\n",
      "Epoch 169, loss: 2.291568\n",
      "Epoch 170, loss: 2.287403\n",
      "Epoch 171, loss: 2.293447\n",
      "Epoch 172, loss: 2.291780\n",
      "Epoch 173, loss: 2.286866\n",
      "Epoch 174, loss: 2.290198\n",
      "Epoch 175, loss: 2.285768\n",
      "Epoch 176, loss: 2.282984\n",
      "Epoch 177, loss: 2.284209\n",
      "Epoch 178, loss: 2.289366\n",
      "Epoch 179, loss: 2.284501\n",
      "Epoch 180, loss: 2.287704\n",
      "Epoch 181, loss: 2.290454\n",
      "Epoch 182, loss: 2.292684\n",
      "Epoch 183, loss: 2.294743\n",
      "Epoch 184, loss: 2.289167\n",
      "Epoch 185, loss: 2.288587\n",
      "Epoch 186, loss: 2.286675\n",
      "Epoch 187, loss: 2.287678\n",
      "Epoch 188, loss: 2.289979\n",
      "Epoch 189, loss: 2.287483\n",
      "Epoch 190, loss: 2.288703\n",
      "Epoch 191, loss: 2.293437\n",
      "Epoch 192, loss: 2.285922\n",
      "Epoch 193, loss: 2.286778\n",
      "Epoch 194, loss: 2.288823\n",
      "Epoch 195, loss: 2.289059\n",
      "Epoch 196, loss: 2.284661\n",
      "Epoch 197, loss: 2.286859\n",
      "Epoch 198, loss: 2.286962\n",
      "Epoch 199, loss: 2.289008\n",
      "Epoch 0, loss: 2.302882\n",
      "Epoch 1, loss: 2.301906\n",
      "Epoch 2, loss: 2.302479\n",
      "Epoch 3, loss: 2.302302\n",
      "Epoch 4, loss: 2.302171\n",
      "Epoch 5, loss: 2.302367\n",
      "Epoch 6, loss: 2.301666\n",
      "Epoch 7, loss: 2.302303\n",
      "Epoch 8, loss: 2.301483\n",
      "Epoch 9, loss: 2.301645\n",
      "Epoch 10, loss: 2.302112\n",
      "Epoch 11, loss: 2.302137\n",
      "Epoch 12, loss: 2.300362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, loss: 2.302166\n",
      "Epoch 14, loss: 2.301141\n",
      "Epoch 15, loss: 2.301223\n",
      "Epoch 16, loss: 2.301609\n",
      "Epoch 17, loss: 2.300870\n",
      "Epoch 18, loss: 2.300707\n",
      "Epoch 19, loss: 2.300849\n",
      "Epoch 20, loss: 2.301745\n",
      "Epoch 21, loss: 2.301313\n",
      "Epoch 22, loss: 2.300453\n",
      "Epoch 23, loss: 2.301603\n",
      "Epoch 24, loss: 2.300738\n",
      "Epoch 25, loss: 2.298830\n",
      "Epoch 26, loss: 2.299370\n",
      "Epoch 27, loss: 2.299568\n",
      "Epoch 28, loss: 2.300524\n",
      "Epoch 29, loss: 2.299748\n",
      "Epoch 30, loss: 2.299932\n",
      "Epoch 31, loss: 2.300551\n",
      "Epoch 32, loss: 2.297319\n",
      "Epoch 33, loss: 2.302134\n",
      "Epoch 34, loss: 2.299764\n",
      "Epoch 35, loss: 2.298723\n",
      "Epoch 36, loss: 2.298532\n",
      "Epoch 37, loss: 2.300918\n",
      "Epoch 38, loss: 2.297807\n",
      "Epoch 39, loss: 2.299111\n",
      "Epoch 40, loss: 2.300583\n",
      "Epoch 41, loss: 2.299240\n",
      "Epoch 42, loss: 2.299506\n",
      "Epoch 43, loss: 2.298735\n",
      "Epoch 44, loss: 2.298080\n",
      "Epoch 45, loss: 2.297365\n",
      "Epoch 46, loss: 2.296308\n",
      "Epoch 47, loss: 2.297939\n",
      "Epoch 48, loss: 2.299748\n",
      "Epoch 49, loss: 2.298721\n",
      "Epoch 50, loss: 2.297549\n",
      "Epoch 51, loss: 2.298292\n",
      "Epoch 52, loss: 2.298448\n",
      "Epoch 53, loss: 2.295284\n",
      "Epoch 54, loss: 2.298556\n",
      "Epoch 55, loss: 2.297446\n",
      "Epoch 56, loss: 2.298671\n",
      "Epoch 57, loss: 2.297649\n",
      "Epoch 58, loss: 2.298376\n",
      "Epoch 59, loss: 2.298220\n",
      "Epoch 60, loss: 2.295458\n",
      "Epoch 61, loss: 2.298969\n",
      "Epoch 62, loss: 2.294913\n",
      "Epoch 63, loss: 2.296844\n",
      "Epoch 64, loss: 2.300842\n",
      "Epoch 65, loss: 2.295419\n",
      "Epoch 66, loss: 2.299007\n",
      "Epoch 67, loss: 2.297516\n",
      "Epoch 68, loss: 2.297117\n",
      "Epoch 69, loss: 2.295330\n",
      "Epoch 70, loss: 2.295399\n",
      "Epoch 71, loss: 2.297070\n",
      "Epoch 72, loss: 2.296523\n",
      "Epoch 73, loss: 2.294842\n",
      "Epoch 74, loss: 2.296248\n",
      "Epoch 75, loss: 2.298388\n",
      "Epoch 76, loss: 2.300323\n",
      "Epoch 77, loss: 2.297014\n",
      "Epoch 78, loss: 2.294103\n",
      "Epoch 79, loss: 2.296723\n",
      "Epoch 80, loss: 2.293431\n",
      "Epoch 81, loss: 2.296085\n",
      "Epoch 82, loss: 2.299504\n",
      "Epoch 83, loss: 2.296237\n",
      "Epoch 84, loss: 2.296951\n",
      "Epoch 85, loss: 2.294740\n",
      "Epoch 86, loss: 2.295080\n",
      "Epoch 87, loss: 2.295991\n",
      "Epoch 88, loss: 2.294460\n",
      "Epoch 89, loss: 2.293563\n",
      "Epoch 90, loss: 2.291634\n",
      "Epoch 91, loss: 2.294265\n",
      "Epoch 92, loss: 2.296667\n",
      "Epoch 93, loss: 2.296348\n",
      "Epoch 94, loss: 2.294121\n",
      "Epoch 95, loss: 2.294275\n",
      "Epoch 96, loss: 2.298126\n",
      "Epoch 97, loss: 2.295060\n",
      "Epoch 98, loss: 2.293424\n",
      "Epoch 99, loss: 2.295745\n",
      "Epoch 100, loss: 2.292979\n",
      "Epoch 101, loss: 2.297060\n",
      "Epoch 102, loss: 2.295791\n",
      "Epoch 103, loss: 2.296883\n",
      "Epoch 104, loss: 2.292563\n",
      "Epoch 105, loss: 2.292236\n",
      "Epoch 106, loss: 2.294696\n",
      "Epoch 107, loss: 2.293405\n",
      "Epoch 108, loss: 2.295961\n",
      "Epoch 109, loss: 2.292994\n",
      "Epoch 110, loss: 2.296641\n",
      "Epoch 111, loss: 2.294555\n",
      "Epoch 112, loss: 2.293603\n",
      "Epoch 113, loss: 2.294678\n",
      "Epoch 114, loss: 2.295653\n",
      "Epoch 115, loss: 2.294403\n",
      "Epoch 116, loss: 2.293054\n",
      "Epoch 117, loss: 2.294696\n",
      "Epoch 118, loss: 2.292888\n",
      "Epoch 119, loss: 2.292644\n",
      "Epoch 120, loss: 2.292420\n",
      "Epoch 121, loss: 2.297726\n",
      "Epoch 122, loss: 2.291801\n",
      "Epoch 123, loss: 2.289867\n",
      "Epoch 124, loss: 2.295232\n",
      "Epoch 125, loss: 2.296787\n",
      "Epoch 126, loss: 2.291851\n",
      "Epoch 127, loss: 2.301268\n",
      "Epoch 128, loss: 2.294722\n",
      "Epoch 129, loss: 2.297793\n",
      "Epoch 130, loss: 2.292370\n",
      "Epoch 131, loss: 2.288229\n",
      "Epoch 132, loss: 2.294736\n",
      "Epoch 133, loss: 2.292038\n",
      "Epoch 134, loss: 2.292858\n",
      "Epoch 135, loss: 2.297928\n",
      "Epoch 136, loss: 2.291484\n",
      "Epoch 137, loss: 2.293914\n",
      "Epoch 138, loss: 2.289732\n",
      "Epoch 139, loss: 2.292697\n",
      "Epoch 140, loss: 2.292018\n",
      "Epoch 141, loss: 2.292930\n",
      "Epoch 142, loss: 2.296296\n",
      "Epoch 143, loss: 2.287004\n",
      "Epoch 144, loss: 2.289531\n",
      "Epoch 145, loss: 2.290598\n",
      "Epoch 146, loss: 2.293626\n",
      "Epoch 147, loss: 2.293703\n",
      "Epoch 148, loss: 2.292726\n",
      "Epoch 149, loss: 2.291187\n",
      "Epoch 150, loss: 2.293465\n",
      "Epoch 151, loss: 2.287881\n",
      "Epoch 152, loss: 2.291391\n",
      "Epoch 153, loss: 2.295466\n",
      "Epoch 154, loss: 2.286902\n",
      "Epoch 155, loss: 2.290518\n",
      "Epoch 156, loss: 2.290054\n",
      "Epoch 157, loss: 2.286768\n",
      "Epoch 158, loss: 2.292275\n",
      "Epoch 159, loss: 2.291571\n",
      "Epoch 160, loss: 2.290047\n",
      "Epoch 161, loss: 2.293442\n",
      "Epoch 162, loss: 2.290384\n",
      "Epoch 163, loss: 2.295664\n",
      "Epoch 164, loss: 2.290582\n",
      "Epoch 165, loss: 2.289134\n",
      "Epoch 166, loss: 2.290857\n",
      "Epoch 167, loss: 2.293278\n",
      "Epoch 168, loss: 2.291334\n",
      "Epoch 169, loss: 2.288063\n",
      "Epoch 170, loss: 2.291460\n",
      "Epoch 171, loss: 2.292836\n",
      "Epoch 172, loss: 2.289245\n",
      "Epoch 173, loss: 2.292981\n",
      "Epoch 174, loss: 2.286792\n",
      "Epoch 175, loss: 2.286211\n",
      "Epoch 176, loss: 2.290386\n",
      "Epoch 177, loss: 2.289144\n",
      "Epoch 178, loss: 2.292055\n",
      "Epoch 179, loss: 2.291652\n",
      "Epoch 180, loss: 2.293228\n",
      "Epoch 181, loss: 2.284860\n",
      "Epoch 182, loss: 2.290865\n",
      "Epoch 183, loss: 2.290696\n",
      "Epoch 184, loss: 2.290961\n",
      "Epoch 185, loss: 2.289132\n",
      "Epoch 186, loss: 2.288971\n",
      "Epoch 187, loss: 2.287014\n",
      "Epoch 188, loss: 2.290475\n",
      "Epoch 189, loss: 2.288979\n",
      "Epoch 190, loss: 2.288349\n",
      "Epoch 191, loss: 2.288038\n",
      "Epoch 192, loss: 2.286263\n",
      "Epoch 193, loss: 2.286677\n",
      "Epoch 194, loss: 2.288086\n",
      "Epoch 195, loss: 2.289662\n",
      "Epoch 196, loss: 2.289845\n",
      "Epoch 197, loss: 2.284778\n",
      "Epoch 198, loss: 2.282696\n",
      "Epoch 199, loss: 2.287764\n",
      "Epoch 0, loss: 2.302739\n",
      "Epoch 1, loss: 2.300105\n",
      "Epoch 2, loss: 2.298493\n",
      "Epoch 3, loss: 2.299005\n",
      "Epoch 4, loss: 2.297128\n",
      "Epoch 5, loss: 2.299685\n",
      "Epoch 6, loss: 2.294705\n",
      "Epoch 7, loss: 2.289737\n",
      "Epoch 8, loss: 2.292416\n",
      "Epoch 9, loss: 2.293287\n",
      "Epoch 10, loss: 2.290480\n",
      "Epoch 11, loss: 2.287144\n",
      "Epoch 12, loss: 2.290794\n",
      "Epoch 13, loss: 2.288445\n",
      "Epoch 14, loss: 2.294272\n",
      "Epoch 15, loss: 2.288797\n",
      "Epoch 16, loss: 2.285868\n",
      "Epoch 17, loss: 2.284510\n",
      "Epoch 18, loss: 2.284839\n",
      "Epoch 19, loss: 2.283921\n",
      "Epoch 20, loss: 2.277685\n",
      "Epoch 21, loss: 2.286232\n",
      "Epoch 22, loss: 2.282934\n",
      "Epoch 23, loss: 2.282391\n",
      "Epoch 24, loss: 2.275806\n",
      "Epoch 25, loss: 2.275774\n",
      "Epoch 26, loss: 2.277349\n",
      "Epoch 27, loss: 2.273381\n",
      "Epoch 28, loss: 2.277045\n",
      "Epoch 29, loss: 2.270021\n",
      "Epoch 30, loss: 2.272601\n",
      "Epoch 31, loss: 2.275681\n",
      "Epoch 32, loss: 2.280180\n",
      "Epoch 33, loss: 2.268088\n",
      "Epoch 34, loss: 2.268811\n",
      "Epoch 35, loss: 2.276251\n",
      "Epoch 36, loss: 2.276104\n",
      "Epoch 37, loss: 2.263146\n",
      "Epoch 38, loss: 2.258129\n",
      "Epoch 39, loss: 2.259229\n",
      "Epoch 40, loss: 2.269855\n",
      "Epoch 41, loss: 2.264732\n",
      "Epoch 42, loss: 2.266250\n",
      "Epoch 43, loss: 2.265283\n",
      "Epoch 44, loss: 2.265407\n",
      "Epoch 45, loss: 2.258409\n",
      "Epoch 46, loss: 2.248692\n",
      "Epoch 47, loss: 2.254799\n",
      "Epoch 48, loss: 2.263780\n",
      "Epoch 49, loss: 2.262245\n",
      "Epoch 50, loss: 2.255613\n",
      "Epoch 51, loss: 2.256671\n",
      "Epoch 52, loss: 2.244041\n",
      "Epoch 53, loss: 2.236192\n",
      "Epoch 54, loss: 2.265471\n",
      "Epoch 55, loss: 2.251657\n",
      "Epoch 56, loss: 2.261829\n",
      "Epoch 57, loss: 2.254525\n",
      "Epoch 58, loss: 2.247519\n",
      "Epoch 59, loss: 2.251996\n",
      "Epoch 60, loss: 2.249877\n",
      "Epoch 61, loss: 2.239710\n",
      "Epoch 62, loss: 2.252110\n",
      "Epoch 63, loss: 2.242823\n",
      "Epoch 64, loss: 2.247001\n",
      "Epoch 65, loss: 2.238181\n",
      "Epoch 66, loss: 2.256686\n",
      "Epoch 67, loss: 2.251325\n",
      "Epoch 68, loss: 2.239763\n",
      "Epoch 69, loss: 2.234495\n",
      "Epoch 70, loss: 2.253759\n",
      "Epoch 71, loss: 2.258742\n",
      "Epoch 72, loss: 2.244396\n",
      "Epoch 73, loss: 2.250096\n",
      "Epoch 74, loss: 2.229219\n",
      "Epoch 75, loss: 2.242283\n",
      "Epoch 76, loss: 2.213873\n",
      "Epoch 77, loss: 2.243800\n",
      "Epoch 78, loss: 2.247001\n",
      "Epoch 79, loss: 2.231375\n",
      "Epoch 80, loss: 2.239445\n",
      "Epoch 81, loss: 2.226213\n",
      "Epoch 82, loss: 2.242950\n",
      "Epoch 83, loss: 2.239754\n",
      "Epoch 84, loss: 2.237354\n",
      "Epoch 85, loss: 2.228543\n",
      "Epoch 86, loss: 2.243519\n",
      "Epoch 87, loss: 2.238678\n",
      "Epoch 88, loss: 2.220785\n",
      "Epoch 89, loss: 2.223237\n",
      "Epoch 90, loss: 2.238667\n",
      "Epoch 91, loss: 2.229997\n",
      "Epoch 92, loss: 2.240358\n",
      "Epoch 93, loss: 2.220594\n",
      "Epoch 94, loss: 2.237389\n",
      "Epoch 95, loss: 2.214501\n",
      "Epoch 96, loss: 2.216770\n",
      "Epoch 97, loss: 2.211915\n",
      "Epoch 98, loss: 2.212098\n",
      "Epoch 99, loss: 2.224207\n",
      "Epoch 100, loss: 2.236946\n",
      "Epoch 101, loss: 2.235711\n",
      "Epoch 102, loss: 2.211216\n",
      "Epoch 103, loss: 2.232672\n",
      "Epoch 104, loss: 2.243510\n",
      "Epoch 105, loss: 2.214900\n",
      "Epoch 106, loss: 2.219809\n",
      "Epoch 107, loss: 2.243979\n",
      "Epoch 108, loss: 2.234299\n",
      "Epoch 109, loss: 2.199628\n",
      "Epoch 110, loss: 2.223560\n",
      "Epoch 111, loss: 2.229972\n",
      "Epoch 112, loss: 2.235174\n",
      "Epoch 113, loss: 2.222056\n",
      "Epoch 114, loss: 2.206370\n",
      "Epoch 115, loss: 2.228202\n",
      "Epoch 116, loss: 2.247452\n",
      "Epoch 117, loss: 2.209733\n",
      "Epoch 118, loss: 2.221991\n",
      "Epoch 119, loss: 2.205004\n",
      "Epoch 120, loss: 2.218889\n",
      "Epoch 121, loss: 2.229324\n",
      "Epoch 122, loss: 2.208036\n",
      "Epoch 123, loss: 2.231187\n",
      "Epoch 124, loss: 2.221618\n",
      "Epoch 125, loss: 2.219962\n",
      "Epoch 126, loss: 2.224451\n",
      "Epoch 127, loss: 2.224315\n",
      "Epoch 128, loss: 2.222667\n",
      "Epoch 129, loss: 2.227847\n",
      "Epoch 130, loss: 2.216278\n",
      "Epoch 131, loss: 2.217141\n",
      "Epoch 132, loss: 2.201165\n",
      "Epoch 133, loss: 2.209500\n",
      "Epoch 134, loss: 2.201397\n",
      "Epoch 135, loss: 2.245389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136, loss: 2.209812\n",
      "Epoch 137, loss: 2.218466\n",
      "Epoch 138, loss: 2.205459\n",
      "Epoch 139, loss: 2.227316\n",
      "Epoch 140, loss: 2.217713\n",
      "Epoch 141, loss: 2.216642\n",
      "Epoch 142, loss: 2.200561\n",
      "Epoch 143, loss: 2.210638\n",
      "Epoch 144, loss: 2.204689\n",
      "Epoch 145, loss: 2.211063\n",
      "Epoch 146, loss: 2.202076\n",
      "Epoch 147, loss: 2.206759\n",
      "Epoch 148, loss: 2.200113\n",
      "Epoch 149, loss: 2.201348\n",
      "Epoch 150, loss: 2.237616\n",
      "Epoch 151, loss: 2.207078\n",
      "Epoch 152, loss: 2.181492\n",
      "Epoch 153, loss: 2.209073\n",
      "Epoch 154, loss: 2.182324\n",
      "Epoch 155, loss: 2.201242\n",
      "Epoch 156, loss: 2.206329\n",
      "Epoch 157, loss: 2.194006\n",
      "Epoch 158, loss: 2.221631\n",
      "Epoch 159, loss: 2.181810\n",
      "Epoch 160, loss: 2.191829\n",
      "Epoch 161, loss: 2.194945\n",
      "Epoch 162, loss: 2.189440\n",
      "Epoch 163, loss: 2.195121\n",
      "Epoch 164, loss: 2.195395\n",
      "Epoch 165, loss: 2.204488\n",
      "Epoch 166, loss: 2.191732\n",
      "Epoch 167, loss: 2.189539\n",
      "Epoch 168, loss: 2.202406\n",
      "Epoch 169, loss: 2.188045\n",
      "Epoch 170, loss: 2.194971\n",
      "Epoch 171, loss: 2.199985\n",
      "Epoch 172, loss: 2.198860\n",
      "Epoch 173, loss: 2.214102\n",
      "Epoch 174, loss: 2.206862\n",
      "Epoch 175, loss: 2.208174\n",
      "Epoch 176, loss: 2.186396\n",
      "Epoch 177, loss: 2.237319\n",
      "Epoch 178, loss: 2.224138\n",
      "Epoch 179, loss: 2.216638\n",
      "Epoch 180, loss: 2.185647\n",
      "Epoch 181, loss: 2.200465\n",
      "Epoch 182, loss: 2.216140\n",
      "Epoch 183, loss: 2.188428\n",
      "Epoch 184, loss: 2.177808\n",
      "Epoch 185, loss: 2.160785\n",
      "Epoch 186, loss: 2.186281\n",
      "Epoch 187, loss: 2.192459\n",
      "Epoch 188, loss: 2.184649\n",
      "Epoch 189, loss: 2.203342\n",
      "Epoch 190, loss: 2.191378\n",
      "Epoch 191, loss: 2.185243\n",
      "Epoch 192, loss: 2.229154\n",
      "Epoch 193, loss: 2.190706\n",
      "Epoch 194, loss: 2.161248\n",
      "Epoch 195, loss: 2.175567\n",
      "Epoch 196, loss: 2.189625\n",
      "Epoch 197, loss: 2.201247\n",
      "Epoch 198, loss: 2.185171\n",
      "Epoch 199, loss: 2.194216\n",
      "Epoch 0, loss: 2.302362\n",
      "Epoch 1, loss: 2.300737\n",
      "Epoch 2, loss: 2.299999\n",
      "Epoch 3, loss: 2.299274\n",
      "Epoch 4, loss: 2.298684\n",
      "Epoch 5, loss: 2.295200\n",
      "Epoch 6, loss: 2.295070\n",
      "Epoch 7, loss: 2.292004\n",
      "Epoch 8, loss: 2.286995\n",
      "Epoch 9, loss: 2.291453\n",
      "Epoch 10, loss: 2.292319\n",
      "Epoch 11, loss: 2.290388\n",
      "Epoch 12, loss: 2.295409\n",
      "Epoch 13, loss: 2.287041\n",
      "Epoch 14, loss: 2.291908\n",
      "Epoch 15, loss: 2.284300\n",
      "Epoch 16, loss: 2.279936\n",
      "Epoch 17, loss: 2.285289\n",
      "Epoch 18, loss: 2.277274\n",
      "Epoch 19, loss: 2.282057\n",
      "Epoch 20, loss: 2.275306\n",
      "Epoch 21, loss: 2.280358\n",
      "Epoch 22, loss: 2.287653\n",
      "Epoch 23, loss: 2.277156\n",
      "Epoch 24, loss: 2.280350\n",
      "Epoch 25, loss: 2.273817\n",
      "Epoch 26, loss: 2.280786\n",
      "Epoch 27, loss: 2.277145\n",
      "Epoch 28, loss: 2.269512\n",
      "Epoch 29, loss: 2.268495\n",
      "Epoch 30, loss: 2.274667\n",
      "Epoch 31, loss: 2.266806\n",
      "Epoch 32, loss: 2.269469\n",
      "Epoch 33, loss: 2.267771\n",
      "Epoch 34, loss: 2.264985\n",
      "Epoch 35, loss: 2.270543\n",
      "Epoch 36, loss: 2.264713\n",
      "Epoch 37, loss: 2.266079\n",
      "Epoch 38, loss: 2.253838\n",
      "Epoch 39, loss: 2.272091\n",
      "Epoch 40, loss: 2.272789\n",
      "Epoch 41, loss: 2.270615\n",
      "Epoch 42, loss: 2.276523\n",
      "Epoch 43, loss: 2.280403\n",
      "Epoch 44, loss: 2.260249\n",
      "Epoch 45, loss: 2.256621\n",
      "Epoch 46, loss: 2.267839\n",
      "Epoch 47, loss: 2.279306\n",
      "Epoch 48, loss: 2.254196\n",
      "Epoch 49, loss: 2.251079\n",
      "Epoch 50, loss: 2.260778\n",
      "Epoch 51, loss: 2.266499\n",
      "Epoch 52, loss: 2.252894\n",
      "Epoch 53, loss: 2.260642\n",
      "Epoch 54, loss: 2.261024\n",
      "Epoch 55, loss: 2.250002\n",
      "Epoch 56, loss: 2.250627\n",
      "Epoch 57, loss: 2.242808\n",
      "Epoch 58, loss: 2.264767\n",
      "Epoch 59, loss: 2.252277\n",
      "Epoch 60, loss: 2.236782\n",
      "Epoch 61, loss: 2.256706\n",
      "Epoch 62, loss: 2.242120\n",
      "Epoch 63, loss: 2.247806\n",
      "Epoch 64, loss: 2.257191\n",
      "Epoch 65, loss: 2.249657\n",
      "Epoch 66, loss: 2.228296\n",
      "Epoch 67, loss: 2.236442\n",
      "Epoch 68, loss: 2.252790\n",
      "Epoch 69, loss: 2.233907\n",
      "Epoch 70, loss: 2.240151\n",
      "Epoch 71, loss: 2.238411\n",
      "Epoch 72, loss: 2.246491\n",
      "Epoch 73, loss: 2.254810\n",
      "Epoch 74, loss: 2.236598\n",
      "Epoch 75, loss: 2.241494\n",
      "Epoch 76, loss: 2.240968\n",
      "Epoch 77, loss: 2.218622\n",
      "Epoch 78, loss: 2.233990\n",
      "Epoch 79, loss: 2.234116\n",
      "Epoch 80, loss: 2.239817\n",
      "Epoch 81, loss: 2.220929\n",
      "Epoch 82, loss: 2.225768\n",
      "Epoch 83, loss: 2.228921\n",
      "Epoch 84, loss: 2.244635\n",
      "Epoch 85, loss: 2.249972\n",
      "Epoch 86, loss: 2.229417\n",
      "Epoch 87, loss: 2.238698\n",
      "Epoch 88, loss: 2.242076\n",
      "Epoch 89, loss: 2.244367\n",
      "Epoch 90, loss: 2.222247\n",
      "Epoch 91, loss: 2.217644\n",
      "Epoch 92, loss: 2.239945\n",
      "Epoch 93, loss: 2.232401\n",
      "Epoch 94, loss: 2.245907\n",
      "Epoch 95, loss: 2.212741\n",
      "Epoch 96, loss: 2.235248\n",
      "Epoch 97, loss: 2.244180\n",
      "Epoch 98, loss: 2.222907\n",
      "Epoch 99, loss: 2.224580\n",
      "Epoch 100, loss: 2.223242\n",
      "Epoch 101, loss: 2.211962\n",
      "Epoch 102, loss: 2.226070\n",
      "Epoch 103, loss: 2.229424\n",
      "Epoch 104, loss: 2.222743\n",
      "Epoch 105, loss: 2.216268\n",
      "Epoch 106, loss: 2.209529\n",
      "Epoch 107, loss: 2.242063\n",
      "Epoch 108, loss: 2.218527\n",
      "Epoch 109, loss: 2.205278\n",
      "Epoch 110, loss: 2.201261\n",
      "Epoch 111, loss: 2.247845\n",
      "Epoch 112, loss: 2.224089\n",
      "Epoch 113, loss: 2.218514\n",
      "Epoch 114, loss: 2.226472\n",
      "Epoch 115, loss: 2.225125\n",
      "Epoch 116, loss: 2.221912\n",
      "Epoch 117, loss: 2.212046\n",
      "Epoch 118, loss: 2.208147\n",
      "Epoch 119, loss: 2.220287\n",
      "Epoch 120, loss: 2.224934\n",
      "Epoch 121, loss: 2.211585\n",
      "Epoch 122, loss: 2.217937\n",
      "Epoch 123, loss: 2.212877\n",
      "Epoch 124, loss: 2.201763\n",
      "Epoch 125, loss: 2.233976\n",
      "Epoch 126, loss: 2.216470\n",
      "Epoch 127, loss: 2.230987\n",
      "Epoch 128, loss: 2.229453\n",
      "Epoch 129, loss: 2.210713\n",
      "Epoch 130, loss: 2.202828\n",
      "Epoch 131, loss: 2.244700\n",
      "Epoch 132, loss: 2.221734\n",
      "Epoch 133, loss: 2.211351\n",
      "Epoch 134, loss: 2.229398\n",
      "Epoch 135, loss: 2.212521\n",
      "Epoch 136, loss: 2.248454\n",
      "Epoch 137, loss: 2.230746\n",
      "Epoch 138, loss: 2.206157\n",
      "Epoch 139, loss: 2.213444\n",
      "Epoch 140, loss: 2.201122\n",
      "Epoch 141, loss: 2.182232\n",
      "Epoch 142, loss: 2.199856\n",
      "Epoch 143, loss: 2.194885\n",
      "Epoch 144, loss: 2.190734\n",
      "Epoch 145, loss: 2.208554\n",
      "Epoch 146, loss: 2.209028\n",
      "Epoch 147, loss: 2.222777\n",
      "Epoch 148, loss: 2.199324\n",
      "Epoch 149, loss: 2.201093\n",
      "Epoch 150, loss: 2.191166\n",
      "Epoch 151, loss: 2.198644\n",
      "Epoch 152, loss: 2.223416\n",
      "Epoch 153, loss: 2.191080\n",
      "Epoch 154, loss: 2.236742\n",
      "Epoch 155, loss: 2.210488\n",
      "Epoch 156, loss: 2.206562\n",
      "Epoch 157, loss: 2.214118\n",
      "Epoch 158, loss: 2.203421\n",
      "Epoch 159, loss: 2.185332\n",
      "Epoch 160, loss: 2.186329\n",
      "Epoch 161, loss: 2.166870\n",
      "Epoch 162, loss: 2.202372\n",
      "Epoch 163, loss: 2.189981\n",
      "Epoch 164, loss: 2.214376\n",
      "Epoch 165, loss: 2.189792\n",
      "Epoch 166, loss: 2.221638\n",
      "Epoch 167, loss: 2.209056\n",
      "Epoch 168, loss: 2.193369\n",
      "Epoch 169, loss: 2.198489\n",
      "Epoch 170, loss: 2.184362\n",
      "Epoch 171, loss: 2.174956\n",
      "Epoch 172, loss: 2.190461\n",
      "Epoch 173, loss: 2.179849\n",
      "Epoch 174, loss: 2.214244\n",
      "Epoch 175, loss: 2.209671\n",
      "Epoch 176, loss: 2.191085\n",
      "Epoch 177, loss: 2.219250\n",
      "Epoch 178, loss: 2.211024\n",
      "Epoch 179, loss: 2.230805\n",
      "Epoch 180, loss: 2.178393\n",
      "Epoch 181, loss: 2.208875\n",
      "Epoch 182, loss: 2.177769\n",
      "Epoch 183, loss: 2.164073\n",
      "Epoch 184, loss: 2.202877\n",
      "Epoch 185, loss: 2.184259\n",
      "Epoch 186, loss: 2.169031\n",
      "Epoch 187, loss: 2.209870\n",
      "Epoch 188, loss: 2.207240\n",
      "Epoch 189, loss: 2.187690\n",
      "Epoch 190, loss: 2.197581\n",
      "Epoch 191, loss: 2.196123\n",
      "Epoch 192, loss: 2.195201\n",
      "Epoch 193, loss: 2.213543\n",
      "Epoch 194, loss: 2.221899\n",
      "Epoch 195, loss: 2.222189\n",
      "Epoch 196, loss: 2.219204\n",
      "Epoch 197, loss: 2.189623\n",
      "Epoch 198, loss: 2.201287\n",
      "Epoch 199, loss: 2.185425\n",
      "Epoch 0, loss: 2.302346\n",
      "Epoch 1, loss: 2.302258\n",
      "Epoch 2, loss: 2.298101\n",
      "Epoch 3, loss: 2.298901\n",
      "Epoch 4, loss: 2.296379\n",
      "Epoch 5, loss: 2.296037\n",
      "Epoch 6, loss: 2.290417\n",
      "Epoch 7, loss: 2.292790\n",
      "Epoch 8, loss: 2.292372\n",
      "Epoch 9, loss: 2.289691\n",
      "Epoch 10, loss: 2.295884\n",
      "Epoch 11, loss: 2.287399\n",
      "Epoch 12, loss: 2.289564\n",
      "Epoch 13, loss: 2.287572\n",
      "Epoch 14, loss: 2.287154\n",
      "Epoch 15, loss: 2.286992\n",
      "Epoch 16, loss: 2.287997\n",
      "Epoch 17, loss: 2.285084\n",
      "Epoch 18, loss: 2.278250\n",
      "Epoch 19, loss: 2.284619\n",
      "Epoch 20, loss: 2.269093\n",
      "Epoch 21, loss: 2.274243\n",
      "Epoch 22, loss: 2.286565\n",
      "Epoch 23, loss: 2.277858\n",
      "Epoch 24, loss: 2.275933\n",
      "Epoch 25, loss: 2.275828\n",
      "Epoch 26, loss: 2.279212\n",
      "Epoch 27, loss: 2.268637\n",
      "Epoch 28, loss: 2.279311\n",
      "Epoch 29, loss: 2.275393\n",
      "Epoch 30, loss: 2.273119\n",
      "Epoch 31, loss: 2.263492\n",
      "Epoch 32, loss: 2.269294\n",
      "Epoch 33, loss: 2.263137\n",
      "Epoch 34, loss: 2.270933\n",
      "Epoch 35, loss: 2.269101\n",
      "Epoch 36, loss: 2.271030\n",
      "Epoch 37, loss: 2.263046\n",
      "Epoch 38, loss: 2.273940\n",
      "Epoch 39, loss: 2.267530\n",
      "Epoch 40, loss: 2.260721\n",
      "Epoch 41, loss: 2.254948\n",
      "Epoch 42, loss: 2.261262\n",
      "Epoch 43, loss: 2.271101\n",
      "Epoch 44, loss: 2.274611\n",
      "Epoch 45, loss: 2.262681\n",
      "Epoch 46, loss: 2.263867\n",
      "Epoch 47, loss: 2.262022\n",
      "Epoch 48, loss: 2.255318\n",
      "Epoch 49, loss: 2.269268\n",
      "Epoch 50, loss: 2.264833\n",
      "Epoch 51, loss: 2.260320\n",
      "Epoch 52, loss: 2.248402\n",
      "Epoch 53, loss: 2.254818\n",
      "Epoch 54, loss: 2.243432\n",
      "Epoch 55, loss: 2.249931\n",
      "Epoch 56, loss: 2.254266\n",
      "Epoch 57, loss: 2.240809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58, loss: 2.250127\n",
      "Epoch 59, loss: 2.248246\n",
      "Epoch 60, loss: 2.249360\n",
      "Epoch 61, loss: 2.254284\n",
      "Epoch 62, loss: 2.247406\n",
      "Epoch 63, loss: 2.258211\n",
      "Epoch 64, loss: 2.247998\n",
      "Epoch 65, loss: 2.258046\n",
      "Epoch 66, loss: 2.268861\n",
      "Epoch 67, loss: 2.236090\n",
      "Epoch 68, loss: 2.241162\n",
      "Epoch 69, loss: 2.239451\n",
      "Epoch 70, loss: 2.235910\n",
      "Epoch 71, loss: 2.239781\n",
      "Epoch 72, loss: 2.235356\n",
      "Epoch 73, loss: 2.251357\n",
      "Epoch 74, loss: 2.234723\n",
      "Epoch 75, loss: 2.255424\n",
      "Epoch 76, loss: 2.242682\n",
      "Epoch 77, loss: 2.232279\n",
      "Epoch 78, loss: 2.225235\n",
      "Epoch 79, loss: 2.250475\n",
      "Epoch 80, loss: 2.234201\n",
      "Epoch 81, loss: 2.236696\n",
      "Epoch 82, loss: 2.233486\n",
      "Epoch 83, loss: 2.231135\n",
      "Epoch 84, loss: 2.244804\n",
      "Epoch 85, loss: 2.242595\n",
      "Epoch 86, loss: 2.235582\n",
      "Epoch 87, loss: 2.251796\n",
      "Epoch 88, loss: 2.236415\n",
      "Epoch 89, loss: 2.234625\n",
      "Epoch 90, loss: 2.228530\n",
      "Epoch 91, loss: 2.237876\n",
      "Epoch 92, loss: 2.203157\n",
      "Epoch 93, loss: 2.246647\n",
      "Epoch 94, loss: 2.224591\n",
      "Epoch 95, loss: 2.227538\n",
      "Epoch 96, loss: 2.235159\n",
      "Epoch 97, loss: 2.236458\n",
      "Epoch 98, loss: 2.216397\n",
      "Epoch 99, loss: 2.229478\n",
      "Epoch 100, loss: 2.234603\n",
      "Epoch 101, loss: 2.248722\n",
      "Epoch 102, loss: 2.223696\n",
      "Epoch 103, loss: 2.221332\n",
      "Epoch 104, loss: 2.206504\n",
      "Epoch 105, loss: 2.242011\n",
      "Epoch 106, loss: 2.241748\n",
      "Epoch 107, loss: 2.228705\n",
      "Epoch 108, loss: 2.234300\n",
      "Epoch 109, loss: 2.256525\n",
      "Epoch 110, loss: 2.212164\n",
      "Epoch 111, loss: 2.209262\n",
      "Epoch 112, loss: 2.239644\n",
      "Epoch 113, loss: 2.234285\n",
      "Epoch 114, loss: 2.227730\n",
      "Epoch 115, loss: 2.200449\n",
      "Epoch 116, loss: 2.257992\n",
      "Epoch 117, loss: 2.218942\n",
      "Epoch 118, loss: 2.228007\n",
      "Epoch 119, loss: 2.204243\n",
      "Epoch 120, loss: 2.223952\n",
      "Epoch 121, loss: 2.206793\n",
      "Epoch 122, loss: 2.208409\n",
      "Epoch 123, loss: 2.218238\n",
      "Epoch 124, loss: 2.219977\n",
      "Epoch 125, loss: 2.200362\n",
      "Epoch 126, loss: 2.215449\n",
      "Epoch 127, loss: 2.197389\n",
      "Epoch 128, loss: 2.233946\n",
      "Epoch 129, loss: 2.193163\n",
      "Epoch 130, loss: 2.223083\n",
      "Epoch 131, loss: 2.192213\n",
      "Epoch 132, loss: 2.213357\n",
      "Epoch 133, loss: 2.221929\n",
      "Epoch 134, loss: 2.202607\n",
      "Epoch 135, loss: 2.199177\n",
      "Epoch 136, loss: 2.212055\n",
      "Epoch 137, loss: 2.219461\n",
      "Epoch 138, loss: 2.214494\n",
      "Epoch 139, loss: 2.223414\n",
      "Epoch 140, loss: 2.171647\n",
      "Epoch 141, loss: 2.208108\n",
      "Epoch 142, loss: 2.208339\n",
      "Epoch 143, loss: 2.188374\n",
      "Epoch 144, loss: 2.213627\n",
      "Epoch 145, loss: 2.188947\n",
      "Epoch 146, loss: 2.204627\n",
      "Epoch 147, loss: 2.192214\n",
      "Epoch 148, loss: 2.215004\n",
      "Epoch 149, loss: 2.179728\n",
      "Epoch 150, loss: 2.207449\n",
      "Epoch 151, loss: 2.218458\n",
      "Epoch 152, loss: 2.212160\n",
      "Epoch 153, loss: 2.203658\n",
      "Epoch 154, loss: 2.192187\n",
      "Epoch 155, loss: 2.190751\n",
      "Epoch 156, loss: 2.204410\n",
      "Epoch 157, loss: 2.189791\n",
      "Epoch 158, loss: 2.193301\n",
      "Epoch 159, loss: 2.216981\n",
      "Epoch 160, loss: 2.200292\n",
      "Epoch 161, loss: 2.180893\n",
      "Epoch 162, loss: 2.195384\n",
      "Epoch 163, loss: 2.192540\n",
      "Epoch 164, loss: 2.183688\n",
      "Epoch 165, loss: 2.190310\n",
      "Epoch 166, loss: 2.177067\n",
      "Epoch 167, loss: 2.219848\n",
      "Epoch 168, loss: 2.212015\n",
      "Epoch 169, loss: 2.196431\n",
      "Epoch 170, loss: 2.205063\n",
      "Epoch 171, loss: 2.216752\n",
      "Epoch 172, loss: 2.180689\n",
      "Epoch 173, loss: 2.202884\n",
      "Epoch 174, loss: 2.192619\n",
      "Epoch 175, loss: 2.203100\n",
      "Epoch 176, loss: 2.200384\n",
      "Epoch 177, loss: 2.193920\n",
      "Epoch 178, loss: 2.202156\n",
      "Epoch 179, loss: 2.202813\n",
      "Epoch 180, loss: 2.193969\n",
      "Epoch 181, loss: 2.179519\n",
      "Epoch 182, loss: 2.177453\n",
      "Epoch 183, loss: 2.215289\n",
      "Epoch 184, loss: 2.232665\n",
      "Epoch 185, loss: 2.202035\n",
      "Epoch 186, loss: 2.185830\n",
      "Epoch 187, loss: 2.207637\n",
      "Epoch 188, loss: 2.222477\n",
      "Epoch 189, loss: 2.204790\n",
      "Epoch 190, loss: 2.204872\n",
      "Epoch 191, loss: 2.176474\n",
      "Epoch 192, loss: 2.189322\n",
      "Epoch 193, loss: 2.213008\n",
      "Epoch 194, loss: 2.195473\n",
      "Epoch 195, loss: 2.197430\n",
      "Epoch 196, loss: 2.206159\n",
      "Epoch 197, loss: 2.195609\n",
      "Epoch 198, loss: 2.195427\n",
      "Epoch 199, loss: 2.188740\n",
      "Epoch 0, loss: 2.302328\n",
      "Epoch 1, loss: 2.303466\n",
      "Epoch 2, loss: 2.303004\n",
      "Epoch 3, loss: 2.303164\n",
      "Epoch 4, loss: 2.303126\n",
      "Epoch 5, loss: 2.302981\n",
      "Epoch 6, loss: 2.303849\n",
      "Epoch 7, loss: 2.302180\n",
      "Epoch 8, loss: 2.303143\n",
      "Epoch 9, loss: 2.303556\n",
      "Epoch 10, loss: 2.302206\n",
      "Epoch 11, loss: 2.302233\n",
      "Epoch 12, loss: 2.302867\n",
      "Epoch 13, loss: 2.303241\n",
      "Epoch 14, loss: 2.302466\n",
      "Epoch 15, loss: 2.303439\n",
      "Epoch 16, loss: 2.303070\n",
      "Epoch 17, loss: 2.302688\n",
      "Epoch 18, loss: 2.302418\n",
      "Epoch 19, loss: 2.302912\n",
      "Epoch 20, loss: 2.302858\n",
      "Epoch 21, loss: 2.302762\n",
      "Epoch 22, loss: 2.303382\n",
      "Epoch 23, loss: 2.302525\n",
      "Epoch 24, loss: 2.302456\n",
      "Epoch 25, loss: 2.302050\n",
      "Epoch 26, loss: 2.302825\n",
      "Epoch 27, loss: 2.302678\n",
      "Epoch 28, loss: 2.302518\n",
      "Epoch 29, loss: 2.303054\n",
      "Epoch 30, loss: 2.302827\n",
      "Epoch 31, loss: 2.301856\n",
      "Epoch 32, loss: 2.302360\n",
      "Epoch 33, loss: 2.302065\n",
      "Epoch 34, loss: 2.302263\n",
      "Epoch 35, loss: 2.301979\n",
      "Epoch 36, loss: 2.301674\n",
      "Epoch 37, loss: 2.302394\n",
      "Epoch 38, loss: 2.303027\n",
      "Epoch 39, loss: 2.303041\n",
      "Epoch 40, loss: 2.301996\n",
      "Epoch 41, loss: 2.302461\n",
      "Epoch 42, loss: 2.301649\n",
      "Epoch 43, loss: 2.302610\n",
      "Epoch 44, loss: 2.302549\n",
      "Epoch 45, loss: 2.302746\n",
      "Epoch 46, loss: 2.302289\n",
      "Epoch 47, loss: 2.302200\n",
      "Epoch 48, loss: 2.302198\n",
      "Epoch 49, loss: 2.301817\n",
      "Epoch 50, loss: 2.301336\n",
      "Epoch 51, loss: 2.302798\n",
      "Epoch 52, loss: 2.302499\n",
      "Epoch 53, loss: 2.302059\n",
      "Epoch 54, loss: 2.301975\n",
      "Epoch 55, loss: 2.302752\n",
      "Epoch 56, loss: 2.301932\n",
      "Epoch 57, loss: 2.301124\n",
      "Epoch 58, loss: 2.301836\n",
      "Epoch 59, loss: 2.303713\n",
      "Epoch 60, loss: 2.302775\n",
      "Epoch 61, loss: 2.302359\n",
      "Epoch 62, loss: 2.302145\n",
      "Epoch 63, loss: 2.302887\n",
      "Epoch 64, loss: 2.303014\n",
      "Epoch 65, loss: 2.302046\n",
      "Epoch 66, loss: 2.302371\n",
      "Epoch 67, loss: 2.301392\n",
      "Epoch 68, loss: 2.301951\n",
      "Epoch 69, loss: 2.302517\n",
      "Epoch 70, loss: 2.302615\n",
      "Epoch 71, loss: 2.302122\n",
      "Epoch 72, loss: 2.302699\n",
      "Epoch 73, loss: 2.303278\n",
      "Epoch 74, loss: 2.302406\n",
      "Epoch 75, loss: 2.302166\n",
      "Epoch 76, loss: 2.302361\n",
      "Epoch 77, loss: 2.301976\n",
      "Epoch 78, loss: 2.301883\n",
      "Epoch 79, loss: 2.301747\n",
      "Epoch 80, loss: 2.301776\n",
      "Epoch 81, loss: 2.301798\n",
      "Epoch 82, loss: 2.301233\n",
      "Epoch 83, loss: 2.301587\n",
      "Epoch 84, loss: 2.302491\n",
      "Epoch 85, loss: 2.302505\n",
      "Epoch 86, loss: 2.301241\n",
      "Epoch 87, loss: 2.301671\n",
      "Epoch 88, loss: 2.301869\n",
      "Epoch 89, loss: 2.302042\n",
      "Epoch 90, loss: 2.302913\n",
      "Epoch 91, loss: 2.302420\n",
      "Epoch 92, loss: 2.302421\n",
      "Epoch 93, loss: 2.302416\n",
      "Epoch 94, loss: 2.302112\n",
      "Epoch 95, loss: 2.301748\n",
      "Epoch 96, loss: 2.302039\n",
      "Epoch 97, loss: 2.301172\n",
      "Epoch 98, loss: 2.301501\n",
      "Epoch 99, loss: 2.301459\n",
      "Epoch 100, loss: 2.301315\n",
      "Epoch 101, loss: 2.302276\n",
      "Epoch 102, loss: 2.302420\n",
      "Epoch 103, loss: 2.303364\n",
      "Epoch 104, loss: 2.302212\n",
      "Epoch 105, loss: 2.301898\n",
      "Epoch 106, loss: 2.302021\n",
      "Epoch 107, loss: 2.302084\n",
      "Epoch 108, loss: 2.301343\n",
      "Epoch 109, loss: 2.302185\n",
      "Epoch 110, loss: 2.302607\n",
      "Epoch 111, loss: 2.302705\n",
      "Epoch 112, loss: 2.302559\n",
      "Epoch 113, loss: 2.302390\n",
      "Epoch 114, loss: 2.303081\n",
      "Epoch 115, loss: 2.302435\n",
      "Epoch 116, loss: 2.302212\n",
      "Epoch 117, loss: 2.301973\n",
      "Epoch 118, loss: 2.301106\n",
      "Epoch 119, loss: 2.302330\n",
      "Epoch 120, loss: 2.301917\n",
      "Epoch 121, loss: 2.300654\n",
      "Epoch 122, loss: 2.302244\n",
      "Epoch 123, loss: 2.300604\n",
      "Epoch 124, loss: 2.302180\n",
      "Epoch 125, loss: 2.301826\n",
      "Epoch 126, loss: 2.303403\n",
      "Epoch 127, loss: 2.302102\n",
      "Epoch 128, loss: 2.302178\n",
      "Epoch 129, loss: 2.301165\n",
      "Epoch 130, loss: 2.301405\n",
      "Epoch 131, loss: 2.300825\n",
      "Epoch 132, loss: 2.302545\n",
      "Epoch 133, loss: 2.301693\n",
      "Epoch 134, loss: 2.301755\n",
      "Epoch 135, loss: 2.301238\n",
      "Epoch 136, loss: 2.300978\n",
      "Epoch 137, loss: 2.302234\n",
      "Epoch 138, loss: 2.301739\n",
      "Epoch 139, loss: 2.301434\n",
      "Epoch 140, loss: 2.300644\n",
      "Epoch 141, loss: 2.301110\n",
      "Epoch 142, loss: 2.300465\n",
      "Epoch 143, loss: 2.302173\n",
      "Epoch 144, loss: 2.302168\n",
      "Epoch 145, loss: 2.301568\n",
      "Epoch 146, loss: 2.301685\n",
      "Epoch 147, loss: 2.300593\n",
      "Epoch 148, loss: 2.302288\n",
      "Epoch 149, loss: 2.302399\n",
      "Epoch 150, loss: 2.301942\n",
      "Epoch 151, loss: 2.301356\n",
      "Epoch 152, loss: 2.302008\n",
      "Epoch 153, loss: 2.302342\n",
      "Epoch 154, loss: 2.302006\n",
      "Epoch 155, loss: 2.301884\n",
      "Epoch 156, loss: 2.301562\n",
      "Epoch 157, loss: 2.301183\n",
      "Epoch 158, loss: 2.301257\n",
      "Epoch 159, loss: 2.301547\n",
      "Epoch 160, loss: 2.301405\n",
      "Epoch 161, loss: 2.302577\n",
      "Epoch 162, loss: 2.300648\n",
      "Epoch 163, loss: 2.300959\n",
      "Epoch 164, loss: 2.301191\n",
      "Epoch 165, loss: 2.300674\n",
      "Epoch 166, loss: 2.301353\n",
      "Epoch 167, loss: 2.300778\n",
      "Epoch 168, loss: 2.301425\n",
      "Epoch 169, loss: 2.301824\n",
      "Epoch 170, loss: 2.302080\n",
      "Epoch 171, loss: 2.300630\n",
      "Epoch 172, loss: 2.301525\n",
      "Epoch 173, loss: 2.300297\n",
      "Epoch 174, loss: 2.300895\n",
      "Epoch 175, loss: 2.300000\n",
      "Epoch 176, loss: 2.302908\n",
      "Epoch 177, loss: 2.301534\n",
      "Epoch 178, loss: 2.301001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179, loss: 2.301686\n",
      "Epoch 180, loss: 2.300829\n",
      "Epoch 181, loss: 2.301255\n",
      "Epoch 182, loss: 2.299917\n",
      "Epoch 183, loss: 2.302009\n",
      "Epoch 184, loss: 2.302303\n",
      "Epoch 185, loss: 2.301375\n",
      "Epoch 186, loss: 2.301920\n",
      "Epoch 187, loss: 2.300721\n",
      "Epoch 188, loss: 2.301316\n",
      "Epoch 189, loss: 2.301348\n",
      "Epoch 190, loss: 2.300453\n",
      "Epoch 191, loss: 2.301507\n",
      "Epoch 192, loss: 2.301076\n",
      "Epoch 193, loss: 2.301158\n",
      "Epoch 194, loss: 2.300210\n",
      "Epoch 195, loss: 2.301888\n",
      "Epoch 196, loss: 2.302295\n",
      "Epoch 197, loss: 2.301109\n",
      "Epoch 198, loss: 2.300896\n",
      "Epoch 199, loss: 2.301937\n",
      "Epoch 0, loss: 2.302750\n",
      "Epoch 1, loss: 2.302389\n",
      "Epoch 2, loss: 2.301329\n",
      "Epoch 3, loss: 2.302570\n",
      "Epoch 4, loss: 2.302407\n",
      "Epoch 5, loss: 2.301921\n",
      "Epoch 6, loss: 2.301012\n",
      "Epoch 7, loss: 2.302821\n",
      "Epoch 8, loss: 2.301780\n",
      "Epoch 9, loss: 2.302164\n",
      "Epoch 10, loss: 2.302016\n",
      "Epoch 11, loss: 2.301604\n",
      "Epoch 12, loss: 2.302136\n",
      "Epoch 13, loss: 2.302486\n",
      "Epoch 14, loss: 2.302629\n",
      "Epoch 15, loss: 2.303451\n",
      "Epoch 16, loss: 2.302103\n",
      "Epoch 17, loss: 2.302348\n",
      "Epoch 18, loss: 2.301870\n",
      "Epoch 19, loss: 2.302321\n",
      "Epoch 20, loss: 2.301499\n",
      "Epoch 21, loss: 2.302767\n",
      "Epoch 22, loss: 2.301407\n",
      "Epoch 23, loss: 2.302183\n",
      "Epoch 24, loss: 2.301586\n",
      "Epoch 25, loss: 2.302140\n",
      "Epoch 26, loss: 2.302593\n",
      "Epoch 27, loss: 2.301952\n",
      "Epoch 28, loss: 2.302006\n",
      "Epoch 29, loss: 2.302633\n",
      "Epoch 30, loss: 2.301841\n",
      "Epoch 31, loss: 2.303320\n",
      "Epoch 32, loss: 2.302651\n",
      "Epoch 33, loss: 2.301575\n",
      "Epoch 34, loss: 2.302184\n",
      "Epoch 35, loss: 2.302096\n",
      "Epoch 36, loss: 2.302096\n",
      "Epoch 37, loss: 2.301912\n",
      "Epoch 38, loss: 2.302428\n",
      "Epoch 39, loss: 2.301772\n",
      "Epoch 40, loss: 2.302130\n",
      "Epoch 41, loss: 2.300674\n",
      "Epoch 42, loss: 2.302159\n",
      "Epoch 43, loss: 2.302889\n",
      "Epoch 44, loss: 2.302035\n",
      "Epoch 45, loss: 2.301440\n",
      "Epoch 46, loss: 2.301911\n",
      "Epoch 47, loss: 2.302260\n",
      "Epoch 48, loss: 2.301187\n",
      "Epoch 49, loss: 2.301660\n",
      "Epoch 50, loss: 2.302288\n",
      "Epoch 51, loss: 2.301765\n",
      "Epoch 52, loss: 2.302389\n",
      "Epoch 53, loss: 2.302009\n",
      "Epoch 54, loss: 2.301716\n",
      "Epoch 55, loss: 2.302106\n",
      "Epoch 56, loss: 2.302467\n",
      "Epoch 57, loss: 2.301969\n",
      "Epoch 58, loss: 2.301607\n",
      "Epoch 59, loss: 2.302502\n",
      "Epoch 60, loss: 2.300932\n",
      "Epoch 61, loss: 2.302685\n",
      "Epoch 62, loss: 2.301302\n",
      "Epoch 63, loss: 2.302225\n",
      "Epoch 64, loss: 2.301998\n",
      "Epoch 65, loss: 2.301508\n",
      "Epoch 66, loss: 2.301147\n",
      "Epoch 67, loss: 2.302184\n",
      "Epoch 68, loss: 2.301891\n",
      "Epoch 69, loss: 2.302110\n",
      "Epoch 70, loss: 2.301366\n",
      "Epoch 71, loss: 2.301351\n",
      "Epoch 72, loss: 2.301250\n",
      "Epoch 73, loss: 2.300999\n",
      "Epoch 74, loss: 2.301817\n",
      "Epoch 75, loss: 2.302264\n",
      "Epoch 76, loss: 2.301728\n",
      "Epoch 77, loss: 2.301185\n",
      "Epoch 78, loss: 2.301130\n",
      "Epoch 79, loss: 2.301125\n",
      "Epoch 80, loss: 2.300148\n",
      "Epoch 81, loss: 2.301497\n",
      "Epoch 82, loss: 2.301135\n",
      "Epoch 83, loss: 2.302235\n",
      "Epoch 84, loss: 2.301489\n",
      "Epoch 85, loss: 2.301579\n",
      "Epoch 86, loss: 2.301548\n",
      "Epoch 87, loss: 2.301694\n",
      "Epoch 88, loss: 2.302423\n",
      "Epoch 89, loss: 2.302063\n",
      "Epoch 90, loss: 2.302734\n",
      "Epoch 91, loss: 2.302255\n",
      "Epoch 92, loss: 2.301993\n",
      "Epoch 93, loss: 2.301790\n",
      "Epoch 94, loss: 2.301835\n",
      "Epoch 95, loss: 2.301115\n",
      "Epoch 96, loss: 2.302090\n",
      "Epoch 97, loss: 2.300570\n",
      "Epoch 98, loss: 2.301594\n",
      "Epoch 99, loss: 2.302174\n",
      "Epoch 100, loss: 2.302257\n",
      "Epoch 101, loss: 2.301117\n",
      "Epoch 102, loss: 2.301664\n",
      "Epoch 103, loss: 2.301593\n",
      "Epoch 104, loss: 2.300570\n",
      "Epoch 105, loss: 2.300779\n",
      "Epoch 106, loss: 2.302118\n",
      "Epoch 107, loss: 2.301815\n",
      "Epoch 108, loss: 2.302230\n",
      "Epoch 109, loss: 2.301232\n",
      "Epoch 110, loss: 2.300118\n",
      "Epoch 111, loss: 2.301215\n",
      "Epoch 112, loss: 2.301971\n",
      "Epoch 113, loss: 2.302060\n",
      "Epoch 114, loss: 2.301467\n",
      "Epoch 115, loss: 2.301130\n",
      "Epoch 116, loss: 2.301245\n",
      "Epoch 117, loss: 2.301884\n",
      "Epoch 118, loss: 2.301072\n",
      "Epoch 119, loss: 2.301394\n",
      "Epoch 120, loss: 2.300986\n",
      "Epoch 121, loss: 2.302975\n",
      "Epoch 122, loss: 2.301842\n",
      "Epoch 123, loss: 2.300579\n",
      "Epoch 124, loss: 2.300733\n",
      "Epoch 125, loss: 2.302121\n",
      "Epoch 126, loss: 2.300026\n",
      "Epoch 127, loss: 2.300414\n",
      "Epoch 128, loss: 2.301125\n",
      "Epoch 129, loss: 2.300552\n",
      "Epoch 130, loss: 2.301471\n",
      "Epoch 131, loss: 2.301645\n",
      "Epoch 132, loss: 2.301683\n",
      "Epoch 133, loss: 2.302781\n",
      "Epoch 134, loss: 2.301949\n",
      "Epoch 135, loss: 2.301351\n",
      "Epoch 136, loss: 2.301589\n",
      "Epoch 137, loss: 2.301274\n",
      "Epoch 138, loss: 2.301447\n",
      "Epoch 139, loss: 2.301424\n",
      "Epoch 140, loss: 2.300751\n",
      "Epoch 141, loss: 2.300673\n",
      "Epoch 142, loss: 2.299495\n",
      "Epoch 143, loss: 2.301432\n",
      "Epoch 144, loss: 2.300941\n",
      "Epoch 145, loss: 2.300797\n",
      "Epoch 146, loss: 2.301562\n",
      "Epoch 147, loss: 2.301296\n",
      "Epoch 148, loss: 2.300763\n",
      "Epoch 149, loss: 2.301452\n",
      "Epoch 150, loss: 2.300595\n",
      "Epoch 151, loss: 2.301004\n",
      "Epoch 152, loss: 2.301396\n",
      "Epoch 153, loss: 2.300769\n",
      "Epoch 154, loss: 2.301446\n",
      "Epoch 155, loss: 2.301004\n",
      "Epoch 156, loss: 2.302268\n",
      "Epoch 157, loss: 2.301714\n",
      "Epoch 158, loss: 2.301129\n",
      "Epoch 159, loss: 2.300573\n",
      "Epoch 160, loss: 2.300583\n",
      "Epoch 161, loss: 2.301369\n",
      "Epoch 162, loss: 2.300873\n",
      "Epoch 163, loss: 2.301810\n",
      "Epoch 164, loss: 2.300864\n",
      "Epoch 165, loss: 2.300967\n",
      "Epoch 166, loss: 2.300343\n",
      "Epoch 167, loss: 2.300542\n",
      "Epoch 168, loss: 2.300855\n",
      "Epoch 169, loss: 2.301121\n",
      "Epoch 170, loss: 2.300815\n",
      "Epoch 171, loss: 2.301298\n",
      "Epoch 172, loss: 2.301272\n",
      "Epoch 173, loss: 2.300676\n",
      "Epoch 174, loss: 2.300586\n",
      "Epoch 175, loss: 2.301885\n",
      "Epoch 176, loss: 2.302259\n",
      "Epoch 177, loss: 2.301140\n",
      "Epoch 178, loss: 2.301810\n",
      "Epoch 179, loss: 2.301002\n",
      "Epoch 180, loss: 2.299262\n",
      "Epoch 181, loss: 2.300559\n",
      "Epoch 182, loss: 2.298901\n",
      "Epoch 183, loss: 2.300866\n",
      "Epoch 184, loss: 2.299867\n",
      "Epoch 185, loss: 2.301161\n",
      "Epoch 186, loss: 2.299727\n",
      "Epoch 187, loss: 2.298835\n",
      "Epoch 188, loss: 2.300302\n",
      "Epoch 189, loss: 2.301352\n",
      "Epoch 190, loss: 2.299625\n",
      "Epoch 191, loss: 2.301518\n",
      "Epoch 192, loss: 2.300534\n",
      "Epoch 193, loss: 2.300185\n",
      "Epoch 194, loss: 2.301821\n",
      "Epoch 195, loss: 2.300248\n",
      "Epoch 196, loss: 2.299981\n",
      "Epoch 197, loss: 2.301699\n",
      "Epoch 198, loss: 2.301569\n",
      "Epoch 199, loss: 2.300968\n",
      "Epoch 0, loss: 2.303135\n",
      "Epoch 1, loss: 2.302480\n",
      "Epoch 2, loss: 2.302091\n",
      "Epoch 3, loss: 2.302521\n",
      "Epoch 4, loss: 2.302737\n",
      "Epoch 5, loss: 2.303319\n",
      "Epoch 6, loss: 2.302363\n",
      "Epoch 7, loss: 2.302276\n",
      "Epoch 8, loss: 2.303314\n",
      "Epoch 9, loss: 2.303427\n",
      "Epoch 10, loss: 2.301777\n",
      "Epoch 11, loss: 2.302186\n",
      "Epoch 12, loss: 2.302517\n",
      "Epoch 13, loss: 2.301999\n",
      "Epoch 14, loss: 2.302630\n",
      "Epoch 15, loss: 2.302720\n",
      "Epoch 16, loss: 2.303032\n",
      "Epoch 17, loss: 2.301828\n",
      "Epoch 18, loss: 2.302755\n",
      "Epoch 19, loss: 2.302831\n",
      "Epoch 20, loss: 2.302489\n",
      "Epoch 21, loss: 2.302890\n",
      "Epoch 22, loss: 2.302013\n",
      "Epoch 23, loss: 2.302466\n",
      "Epoch 24, loss: 2.302661\n",
      "Epoch 25, loss: 2.302041\n",
      "Epoch 26, loss: 2.302252\n",
      "Epoch 27, loss: 2.302801\n",
      "Epoch 28, loss: 2.301993\n",
      "Epoch 29, loss: 2.302068\n",
      "Epoch 30, loss: 2.302239\n",
      "Epoch 31, loss: 2.302779\n",
      "Epoch 32, loss: 2.302085\n",
      "Epoch 33, loss: 2.301958\n",
      "Epoch 34, loss: 2.301834\n",
      "Epoch 35, loss: 2.302562\n",
      "Epoch 36, loss: 2.302784\n",
      "Epoch 37, loss: 2.301322\n",
      "Epoch 38, loss: 2.301703\n",
      "Epoch 39, loss: 2.301878\n",
      "Epoch 40, loss: 2.301110\n",
      "Epoch 41, loss: 2.303015\n",
      "Epoch 42, loss: 2.301704\n",
      "Epoch 43, loss: 2.301545\n",
      "Epoch 44, loss: 2.303066\n",
      "Epoch 45, loss: 2.302485\n",
      "Epoch 46, loss: 2.301684\n",
      "Epoch 47, loss: 2.302676\n",
      "Epoch 48, loss: 2.301910\n",
      "Epoch 49, loss: 2.301697\n",
      "Epoch 50, loss: 2.302596\n",
      "Epoch 51, loss: 2.301833\n",
      "Epoch 52, loss: 2.302188\n",
      "Epoch 53, loss: 2.301977\n",
      "Epoch 54, loss: 2.302930\n",
      "Epoch 55, loss: 2.302074\n",
      "Epoch 56, loss: 2.302835\n",
      "Epoch 57, loss: 2.302009\n",
      "Epoch 58, loss: 2.303174\n",
      "Epoch 59, loss: 2.301960\n",
      "Epoch 60, loss: 2.302605\n",
      "Epoch 61, loss: 2.301725\n",
      "Epoch 62, loss: 2.301524\n",
      "Epoch 63, loss: 2.302719\n",
      "Epoch 64, loss: 2.302429\n",
      "Epoch 65, loss: 2.301857\n",
      "Epoch 66, loss: 2.301272\n",
      "Epoch 67, loss: 2.302680\n",
      "Epoch 68, loss: 2.302005\n",
      "Epoch 69, loss: 2.301996\n",
      "Epoch 70, loss: 2.302302\n",
      "Epoch 71, loss: 2.301421\n",
      "Epoch 72, loss: 2.301995\n",
      "Epoch 73, loss: 2.301457\n",
      "Epoch 74, loss: 2.301030\n",
      "Epoch 75, loss: 2.301574\n",
      "Epoch 76, loss: 2.302305\n",
      "Epoch 77, loss: 2.302135\n",
      "Epoch 78, loss: 2.302361\n",
      "Epoch 79, loss: 2.301489\n",
      "Epoch 80, loss: 2.302318\n",
      "Epoch 81, loss: 2.301174\n",
      "Epoch 82, loss: 2.301514\n",
      "Epoch 83, loss: 2.301709\n",
      "Epoch 84, loss: 2.301971\n",
      "Epoch 85, loss: 2.301254\n",
      "Epoch 86, loss: 2.300888\n",
      "Epoch 87, loss: 2.301494\n",
      "Epoch 88, loss: 2.301176\n",
      "Epoch 89, loss: 2.301174\n",
      "Epoch 90, loss: 2.301844\n",
      "Epoch 91, loss: 2.301949\n",
      "Epoch 92, loss: 2.301708\n",
      "Epoch 93, loss: 2.302594\n",
      "Epoch 94, loss: 2.300038\n",
      "Epoch 95, loss: 2.301531\n",
      "Epoch 96, loss: 2.302074\n",
      "Epoch 97, loss: 2.301835\n",
      "Epoch 98, loss: 2.301518\n",
      "Epoch 99, loss: 2.300761\n",
      "Epoch 100, loss: 2.301650\n",
      "Epoch 101, loss: 2.301740\n",
      "Epoch 102, loss: 2.301516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103, loss: 2.301625\n",
      "Epoch 104, loss: 2.300876\n",
      "Epoch 105, loss: 2.301809\n",
      "Epoch 106, loss: 2.301960\n",
      "Epoch 107, loss: 2.302355\n",
      "Epoch 108, loss: 2.301140\n",
      "Epoch 109, loss: 2.301880\n",
      "Epoch 110, loss: 2.302625\n",
      "Epoch 111, loss: 2.301400\n",
      "Epoch 112, loss: 2.301762\n",
      "Epoch 113, loss: 2.300949\n",
      "Epoch 114, loss: 2.301539\n",
      "Epoch 115, loss: 2.301293\n",
      "Epoch 116, loss: 2.301956\n",
      "Epoch 117, loss: 2.301649\n",
      "Epoch 118, loss: 2.301878\n",
      "Epoch 119, loss: 2.302165\n",
      "Epoch 120, loss: 2.301468\n",
      "Epoch 121, loss: 2.301229\n",
      "Epoch 122, loss: 2.300604\n",
      "Epoch 123, loss: 2.301791\n",
      "Epoch 124, loss: 2.301858\n",
      "Epoch 125, loss: 2.302442\n",
      "Epoch 126, loss: 2.300341\n",
      "Epoch 127, loss: 2.301201\n",
      "Epoch 128, loss: 2.301307\n",
      "Epoch 129, loss: 2.301267\n",
      "Epoch 130, loss: 2.301888\n",
      "Epoch 131, loss: 2.301058\n",
      "Epoch 132, loss: 2.300966\n",
      "Epoch 133, loss: 2.302189\n",
      "Epoch 134, loss: 2.300724\n",
      "Epoch 135, loss: 2.300519\n",
      "Epoch 136, loss: 2.300402\n",
      "Epoch 137, loss: 2.300226\n",
      "Epoch 138, loss: 2.302041\n",
      "Epoch 139, loss: 2.302741\n",
      "Epoch 140, loss: 2.300772\n",
      "Epoch 141, loss: 2.302156\n",
      "Epoch 142, loss: 2.303043\n",
      "Epoch 143, loss: 2.301833\n",
      "Epoch 144, loss: 2.300888\n",
      "Epoch 145, loss: 2.301170\n",
      "Epoch 146, loss: 2.300859\n",
      "Epoch 147, loss: 2.301819\n",
      "Epoch 148, loss: 2.300545\n",
      "Epoch 149, loss: 2.302234\n",
      "Epoch 150, loss: 2.300238\n",
      "Epoch 151, loss: 2.301898\n",
      "Epoch 152, loss: 2.301411\n",
      "Epoch 153, loss: 2.301958\n",
      "Epoch 154, loss: 2.300401\n",
      "Epoch 155, loss: 2.300879\n",
      "Epoch 156, loss: 2.302214\n",
      "Epoch 157, loss: 2.301317\n",
      "Epoch 158, loss: 2.301187\n",
      "Epoch 159, loss: 2.301566\n",
      "Epoch 160, loss: 2.300398\n",
      "Epoch 161, loss: 2.301686\n",
      "Epoch 162, loss: 2.301599\n",
      "Epoch 163, loss: 2.301319\n",
      "Epoch 164, loss: 2.299968\n",
      "Epoch 165, loss: 2.300850\n",
      "Epoch 166, loss: 2.301828\n",
      "Epoch 167, loss: 2.300623\n",
      "Epoch 168, loss: 2.301478\n",
      "Epoch 169, loss: 2.301110\n",
      "Epoch 170, loss: 2.300686\n",
      "Epoch 171, loss: 2.300701\n",
      "Epoch 172, loss: 2.302151\n",
      "Epoch 173, loss: 2.301377\n",
      "Epoch 174, loss: 2.301522\n",
      "Epoch 175, loss: 2.300676\n",
      "Epoch 176, loss: 2.300827\n",
      "Epoch 177, loss: 2.301074\n",
      "Epoch 178, loss: 2.301511\n",
      "Epoch 179, loss: 2.300984\n",
      "Epoch 180, loss: 2.299588\n",
      "Epoch 181, loss: 2.301877\n",
      "Epoch 182, loss: 2.301286\n",
      "Epoch 183, loss: 2.302493\n",
      "Epoch 184, loss: 2.301650\n",
      "Epoch 185, loss: 2.300322\n",
      "Epoch 186, loss: 2.301011\n",
      "Epoch 187, loss: 2.299771\n",
      "Epoch 188, loss: 2.302896\n",
      "Epoch 189, loss: 2.301482\n",
      "Epoch 190, loss: 2.300311\n",
      "Epoch 191, loss: 2.300944\n",
      "Epoch 192, loss: 2.300864\n",
      "Epoch 193, loss: 2.300318\n",
      "Epoch 194, loss: 2.301570\n",
      "Epoch 195, loss: 2.300323\n",
      "Epoch 196, loss: 2.300484\n",
      "Epoch 197, loss: 2.301086\n",
      "Epoch 198, loss: 2.300496\n",
      "Epoch 199, loss: 2.299405\n",
      "best validation accuracy achieved: 0.228333\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "\n",
    "learning_rates = [1e-3, 2e-3,1e-4, 15e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "num_train = train_X.shape[0]\n",
    "num_val = train_X.shape[0]//5\n",
    "shuffled_indices = np.arange(num_train)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "sections = np.arange(num_val, num_train, num_val)\n",
    "batches_indices = np.array_split(shuffled_indices, sections)\n",
    "\n",
    "train_val_X = train_X[np.concatenate(batches_indices[:4])]\n",
    "test_val_X = train_X[batches_indices[4]]\n",
    "\n",
    "train_val_y = train_y[np.concatenate(batches_indices[:4])]\n",
    "test_val_y = train_y[batches_indices[4]]\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "for i in learning_rates:\n",
    "    for j in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_val_X, train_val_y, batch_size, i, j, num_epochs)\n",
    "        pred = classifier.predict(test_val_X)\n",
    "        accuracy = multiclass_accuracy(pred, test_val_y)\n",
    "        if accuracy >= best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "        \n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.203000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
